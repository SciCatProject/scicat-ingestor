# SPDX-License-Identifier: BSD-3-Clause
# Copyright (c) 2024 ScicatProject contributors (https://github.com/ScicatProject)
import argparse
import json
import logging
from collections.abc import Mapping
from dataclasses import asdict, dataclass, field, is_dataclass
from functools import partial
from inspect import get_annotations
from pathlib import Path
from types import MappingProxyType
from typing import Any, TypeVar, get_origin
from urllib.parse import urljoin

import yaml


def _is_json_file(text: str) -> bool:
    """Check if the text is a valid JSON file."""
    try:
        json.loads(text)
        return True
    except json.JSONDecodeError:
        return False


def _load_config(config_file: Path) -> dict:
    """Load configuration from the configuration file path."""
    import warnings

    if config_file.is_file():
        text = config_file.read_text()
        if _is_json_file(text):
            # If it is a JSON file, load it as JSON.
            # We cannot use try-except here since `yaml.safe_load`
            # can also load JSON strings.
            warnings.warn(
                "The json configuration file format is deprecated. Please use YAML format.\n"
                "You can dump the configuration to YAML using `scicat-json-to-yaml` command."
                "This warning will be removed from future versions.",
                DeprecationWarning,
                stacklevel=3,
            )
        loaded_config = yaml.safe_load(text)
    else:
        raise FileNotFoundError(f"Configuration file not found: {config_file}")

    if not isinstance(loaded_config, dict):
        raise ValueError(
            f"Dictionary configuration was expected but got {type(loaded_config).__name__}."
        )
    return loaded_config


def _insert_item(d: dict, key: str, value: Any) -> None:
    """Insert a key-value pair into a dictionary.

    If ``key`` is a nested key, the function creates the nested dictionary.

    Example:
    >>> d = {}
    >>> _insert_item(d, "a.b.c", 1)
    >>> d
    {'a': {'b': {'c': 1}}}
    """
    key_parts = key.split(".")
    for key_part in key_parts[:-1]:
        d = d.setdefault(key_part, {})
    d[key_parts[-1]] = value


def _parse_nested_input_args(input_args: argparse.Namespace) -> dict:
    nested_args = {}
    for key, value in vars(input_args).items():
        _insert_item(nested_args, key, value)
    return nested_args


_SHORTENED_ARG_NAMES = MappingProxyType(
    {
        "config-file": "c",
        "ingestion.dry-run": "d",
    }
)

file_handling_help = {
    "data-file-open-max-tries": "How many times to try opening a data file before giving up.",
    "data-file-open-retry-delay": "A list of retry delays for opening a data file. "
    "If a shorter list is provided than ``data-file-open-max-retries``, "
    "the last value will be used for all remaining retries. "
    "If the list is empty, the default value of 3 seconds will be used for all retries. "
    "Retry delay should be between 1 and 15 seconds. "
    "Any number outside this range will be clamped to the nearest boundary.",
}

_HELP_TEXT = {
    "config-file": "Path to the configuration file.",
    "ingestion.dry-run": "Dry run mode. No data will be sent to SciCat.",
    **{
        "ingestion.file-handling." + key: value
        for key, value in file_handling_help.items()
    },
}


def _wrap_arg_names(name: str, *prefixes: str) -> tuple[str, ...]:
    long_name = (".".join((*prefixes, name)) if prefixes else name).replace("_", "-")
    long_arg_name = "--" + long_name
    return (
        (long_arg_name,)
        if (short_arg_name := _SHORTENED_ARG_NAMES.get(long_name)) is None
        else ("-" + short_arg_name, long_arg_name)
    )


def build_arg_parser(
    config_dataclass: type, mandatory_args: tuple[str, ...] = ()
) -> argparse.ArgumentParser:
    """Build an argument parser from a dataclass.

    **Note**: It can't parse the annotations from parent class.
    """
    parser = argparse.ArgumentParser(argument_default=argparse.SUPPRESS)

    def _add_arguments(dataclass_tp: type, prefixes: tuple[str, ...] = ()) -> None:
        # Add argument group if prefixes are provided
        if len(prefixes) > 0:
            group_name = " ".join(
                prefix.replace("_", " ").capitalize() for prefix in prefixes
            )
            group = parser.add_argument_group(group_name)
        else:
            group = parser

        all_types = get_annotations(dataclass_tp)
        # Add arguments for atomic types
        atomic_types = {
            name: tp for name, tp in all_types.items() if not is_dataclass(tp)
        }

        for name, tp in atomic_types.items():
            arg_names = _wrap_arg_names(name, *prefixes)
            required = any(arg_name in mandatory_args for arg_name in arg_names)
            long_name = arg_names[-1].replace("--", "")
            arg_adder = partial(
                group.add_argument,
                *arg_names,
                required=required,
                help=_HELP_TEXT.get(long_name),
            )
            if tp is bool:
                arg_adder(action="store_true")
            elif tp in (int, float, str):
                arg_adder(type=tp)
            elif (
                tp is list
                or tp is tuple
                or (orig := get_origin(tp)) is list
                or orig is tuple
            ):
                arg_adder(nargs="+")
            elif tp is dict:
                ...  # dict type is not supported from the command line
            elif tp == str | None:
                arg_adder(type=str)
            else:
                raise ValueError(
                    f"Unsupported type for argument parsing: {tp} in {dataclass_tp}"
                )

        # Recursively add arguments for nested dataclasses
        # It is done separately to use argument groups
        sub_dataclasses = {
            name: tp
            for name, tp in all_types.items()
            if is_dataclass(tp) and isinstance(tp, type)
        }

        for name, tp in sub_dataclasses.items():
            _add_arguments(tp, (*prefixes, name))

    _add_arguments(config_dataclass)
    return parser


def _recursive_deepcopy(obj: Any) -> dict:
    """Recursively deep copy a dictionary."""
    if not isinstance(obj, dict | MappingProxyType):
        return obj

    copied = dict(obj)
    for key, value in copied.items():
        if isinstance(value, Mapping | MappingProxyType):
            copied[key] = _recursive_deepcopy(value)

    return copied


@dataclass(kw_only=True)
class LoggingOptions:
    """
    LoggingOptions dataclass to store the configuration options.

    Most of options don't have default values because they are expected
    to be set by the user either in the configuration file or through
    command line arguments.
    """

    verbose: bool = False
    file_log: bool = False
    file_log_base_name: str = "scicat_ingestor_log"
    file_log_timestamp: bool = False
    logging_level: str = "INFO"
    log_message_prefix: str = "SFI"
    system_log: bool = False
    system_log_facility: str | None = "mail"
    graylog: bool = False
    graylog_host: str = ""
    graylog_port: str = ""
    graylog_facility: str = "scicat.ingestor"


@dataclass(kw_only=True)
class HealthCheckOptions:
    host: str = "0.0.0.0"  # noqa: S104
    port: int = 8080


@dataclass(kw_only=True)
class KafkaOptions:
    """
    KafkaOptions dataclass to store the configuration options.

    Default values are provided as they are not
    expected to be set by command line arguments.
    """

    topics: str = "KAFKA_TOPIC_1,KAFKA_TOPIC_2"
    """List of Kafka topics. Multiple topics can be separated by commas."""
    group_id: str = "GROUP_ID"
    """Kafka consumer group ID."""
    bootstrap_servers: str = "localhost:9093"
    """List of Kafka bootstrap servers. Multiple servers can be separated by commas."""
    security_protocol: str = "sasl_ssl"
    """Security protocol """
    sasl_mechanism: str = "SCRAM-SHA-256"
    """Kafka SASL mechanism."""
    sasl_username: str = "USERNAME"
    """Kafka SASL username."""
    sasl_password: str = "PASSWORD"
    """Kafka SASL password."""
    ssl_ca_location: str = "FULL_PATH_TO_CERTIFICATE_FILE"
    """Kafka SSL CA location."""
    individual_message_commit: bool = True
    """Commit for each topic individually."""
    enable_auto_commit: bool = True
    """Enable Kafka auto commit."""
    auto_offset_reset: str = "earliest"
    """Kafka auto offset reset."""


@dataclass(kw_only=True)
class FileHandlingOptions:
    compute_file_stats: bool = True
    compute_file_hash: bool = True
    file_hash_algorithm: str = "blake2b"
    save_file_hash: bool = True
    hash_file_extension: str = "b2b"
    ingestor_files_directory: str = "../ingestor"
    message_to_file: bool = True
    message_file_extension: str = "message.json"
    file_path_type: str = "relative"  # allowed values: absolute and relative
    data_directory: str = ""
    data_file_open_max_tries: int = 3
    """How many times to try opening a data file before giving up."""
    data_file_open_retry_delay: list[int] = field(default_factory=lambda: [])
    """A list of retry delays for opening a data file.

    It is a list of numbers, each representing a different retry delay per retry.
    If the list is shorter than the number of retries,
    the last value will be used for all remaining retries.
    If the list is longer, the excess values will be ignored.

    If the list is empty, the default value of 3 seconds will be used for all retries.
    Maximum retry delay is 15 seconds, and minimum is 1 second.
    Any number outside this range will be clamped to the nearest boundary.
    """


def default_offline_ingestor_executable() -> list[str]:
    return ["scicat_background_ingestor"]


@dataclass(kw_only=True)
class IngestionOptions:
    dry_run: bool = False
    offline_ingestor_executable: list[str] = field(
        default_factory=default_offline_ingestor_executable
    )
    max_offline_ingestors: int = 10
    offline_ingestors_wait_time: int = 10
    schemas_directory: str = "schemas"
    check_if_dataset_exists_by_pid: bool = True
    check_if_dataset_exists_by_metadata: bool = True
    check_if_dataset_exists_by_metadata_key: str = "job_id"
    file_handling: FileHandlingOptions = field(default_factory=FileHandlingOptions)


def default_access_groups() -> list[str]:
    return ["ACCESS_GROUP_1"]


@dataclass(kw_only=True)
class DatasetOptions:
    allow_dataset_pid: bool = True
    generate_dataset_pid: bool = False
    dataset_pid_prefix: str = "20.500.12269"
    default_instrument_id: str = "ID_OF_FALLBACK_INSTRUMENT"
    default_proposal_id: str = "DEFAULT_PROPOSAL_ID"
    default_owner_group: str = "DEFAULT_OWNER_GROUP"
    default_access_groups: list[str] = field(default_factory=default_access_groups)


@dataclass(kw_only=True)
class _ScicatAPIURLs:
    datasets: str
    proposals: str
    origdatablocks: str
    instruments: str


@dataclass(kw_only=True)
class ScicatEndpoints:
    datasets: str = "datasets"
    proposals: str = "proposals"
    origdatablocks: str = "origdatablocks"
    instruments: str = "instruments"


@dataclass(kw_only=True)
class SciCatOptions:
    host: str = "https://scicat.host"
    token: str = "JWT_TOKEN"
    additional_headers: dict = field(default_factory=dict)
    timeout: int = 0
    stream: bool = True
    verify: bool = False
    health_endpoint: str = "health"
    api_endpoints: ScicatEndpoints = field(default_factory=ScicatEndpoints)

    @property
    def urls(self) -> _ScicatAPIURLs:
        return _ScicatAPIURLs(
            datasets=urljoin(self.host_address, self.api_endpoints.datasets),
            proposals=urljoin(self.host_address, self.api_endpoints.proposals),
            origdatablocks=urljoin(
                self.host_address, self.api_endpoints.origdatablocks
            ),
            instruments=urljoin(self.host_address, self.api_endpoints.instruments),
        )

    @property
    def headers(self) -> dict:
        return {
            **self.additional_headers,
            **{"Authorization": f"Bearer {self.token}"},
        }

    @property
    def host_address(self) -> str:
        """Return the host address ready to be used."""
        return self.host.removesuffix('/') + "/"

    @property
    def health_url(self) -> str:
        """Return the health-check URL, allowing either relative or absolute values."""

        endpoint = self.health_endpoint
        if endpoint.startswith(("http://", "https://")):
            return endpoint
        return urljoin(self.host_address, endpoint.lstrip("/"))


@dataclass(kw_only=True)
class OnlineIngestorConfig:
    # original_dict: Mapping
    """Original configuration dictionary in the json file."""

    nexus_file: str = ""
    done_writing_message_file: str = ""
    config_file: str
    id: str = ""
    dataset: DatasetOptions = field(default_factory=DatasetOptions)
    ingestion: IngestionOptions = field(default_factory=IngestionOptions)
    kafka: KafkaOptions = field(default_factory=KafkaOptions)
    logging: LoggingOptions = field(default_factory=LoggingOptions)
    scicat: SciCatOptions = field(default_factory=SciCatOptions)
    health_check: HealthCheckOptions = field(default_factory=HealthCheckOptions)

    def to_dict(self) -> dict:
        """Return the configuration as a dictionary."""

        return asdict(self)


@dataclass(kw_only=True)
class OfflineIngestorConfig:
    nexus_file: str
    """Full path of the input nexus file to be ingested."""
    done_writing_message_file: str
    """Full path of the done writing message file that match the ``nexus_file``."""
    config_file: str
    id: str
    dataset: DatasetOptions = field(default_factory=DatasetOptions)
    ingestion: IngestionOptions = field(default_factory=IngestionOptions)
    logging: LoggingOptions = field(default_factory=LoggingOptions)
    scicat: SciCatOptions = field(default_factory=SciCatOptions)

    def to_dict(self) -> dict:
        """Return the configuration as a dictionary."""

        return asdict(self)


T = TypeVar("T")


def build_dataclass(
    *,
    tp: type[T],
    data: dict,
    prefixes: tuple[str, ...] = (),
    logger: logging.Logger | None = None,
    strict: bool = False,
) -> T:
    type_hints = get_annotations(tp)
    unused_keys = set(data.keys()) - set(type_hints.keys())
    if unused_keys:
        # If ``data`` contains unnecessary fields.
        unused_keys_repr = "\n\t\t- ".join(
            ".".join((*prefixes, unused_key)) for unused_key in unused_keys
        )
        error_message = f"Invalid argument found: \n\t\t- {unused_keys_repr}"
        if logger is not None:
            logger.warning(error_message)
        if strict:
            raise ValueError(error_message)
    return tp(
        **{
            key: build_dataclass(tp=sub_tp, data=value, prefixes=(*prefixes, key))
            if is_dataclass(sub_tp := type_hints.get(key))
            else value
            for key, value in data.items()
            if key not in unused_keys
        }
    )


def _merge_config_and_input_args(config_dict: dict, input_args_dict: dict) -> dict:
    """Merge nested dictionaries.

    ``input_args_dict`` has higher priority than ``config_dict``.
    """
    return {
        key: _merge_config_and_input_args(
            config_dict.get(key, {}), input_args_dict.get(key, {})
        )
        if (
            isinstance(config_dict.get(key), dict)
            or isinstance(input_args_dict.get(key), dict)
        )
        else i_value
        if (i_value := input_args_dict.get(key)) is not None
        else config_dict.get(key)
        for key in set(config_dict.keys()).union(set(input_args_dict.keys()))
    }


def merge_config_and_input_args(
    config_file: Path, arg_namespace: argparse.Namespace
) -> dict[str, Any]:
    config_from_file = _load_config(config_file)
    return _merge_config_and_input_args(
        config_from_file, _parse_nested_input_args(arg_namespace)
    )


def _validate_config_file(target_type: type[T], config_file: Path) -> T:
    config = {**_load_config(config_file), "config_file": config_file.as_posix()}
    return build_dataclass(tp=target_type, data=config, strict=True)


def validate_config_file() -> None:
    """Validate the configuration file."""
    from scicat_logging import build_devtool_logger

    arg_parser = argparse.ArgumentParser()
    arg_parser.add_argument(
        help="Configuration file path to validate.", dest="config_file"
    )
    config_file = Path(arg_parser.parse_args().config_file)

    logger = build_devtool_logger("json-to-yaml")
    logger.info("Scicat file ingestor configuration file validation test.")
    logger.info("Note that it does not validate type of the field.")
    logger.debug("It only validate the file for %s.", OnlineIngestorConfig)
    logger.debug("Configuration file: %s", config_file)

    if not config_file.is_file():
        raise FileNotFoundError(f"Configuration file not found: {config_file}")
    logger.debug(
        "Configuration built successfully. %s",
        _validate_config_file(OnlineIngestorConfig, config_file),
    )
    logger.info("Configuration file %s is valid.", config_file)


def synchronize_config_file() -> None:
    """Synchronize the configurations from the dataclass and json file."""

    config_file_from_repo = Path("resources/config.sample.yml")
    default_config = OnlineIngestorConfig(config_file="")

    target_file = Path(__file__).parent.parent / config_file_from_repo
    with target_file.open("w") as f_stream:
        yaml.safe_dump(default_config.to_dict(), stream=f_stream, sort_keys=False)


def json_to_yaml() -> None:
    import argparse
    import json
    from pathlib import Path

    import yaml

    from scicat_logging import build_devtool_logger

    logger = build_devtool_logger("json-to-yaml")

    parser = argparse.ArgumentParser(description="Convert JSON to YAML.")
    parser.add_argument(
        "--input-file", type=str, help="Input JSON file.", required=True
    )
    parser.add_argument(
        "--output-file",
        type=str,
        help="Output YAML file. If not specified, "
        "output file path will be the path of the input file "
        "with `.json` replaced by `.yml.`",
        default=None,
    )

    args = parser.parse_args()

    output_file_path = args.output_file
    if output_file_path is None:
        output_file_path = args.input_file.replace(".json", ".yml")

    if (output_file_path := Path(output_file_path)).exists():
        answer = input(
            f"Output file {output_file_path} already exists. Do you want to overwrite it? (y/n): "
        )
        if answer.lower() != "y":
            logger.info("Operation cancelled.")
            return

    with Path(args.input_file).open("r") as json_file:
        json_data = json.load(json_file)

    with output_file_path.open("w") as yaml_file:
        yaml.dump(json_data, yaml_file, sort_keys=False)


if __name__ == "__main__":
    validate_config_file()

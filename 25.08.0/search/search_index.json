{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Welcome to Scicat Ingestor","text":"<p>SciCat Ingestor is a versatile application with the primary focus to automate the ingestion of new dataset in to SciCat.</p> <p>Scicat Ingestor aims to accomplish <code>FAIR</code> data  by making files visible via <code>scicat</code>, associated with their metadata.</p> <p>The project is composed of two main components:</p> <ul> <li> <p>online ingestor</p> <p>is responsible to connect to a kafka cluster and listen to selected topics for a specific message and trigger the data ingestion by running the offline ingestor as a background process. At the moment, this is specific to ESS IT infrastructure, but it is already planned to generalize it as soon as other facilities express interest in adopting it.</p> <p>For details, see online ingestor page.</p> </li> <li> <p>offline ingestor</p> <p>can be run from the online ingestor or by an operator. It is responsible to collect all the necessary metadata and create a dataset entry in SciCat.</p> <p>For details, see offline ingestor page.</p> </li> </ul>"},{"location":"#key-features","title":"Key Features","text":"<ul> <li>Continuously and asynchronously retrieving information of <code>files</code> from kafka.</li> <li>Retrieve metadata from <code>files</code>.</li> <li>Ingest <code>files</code> along with retrieved metadata to <code>scicat</code>.</li> </ul>"},{"location":"#infrastructure-around-scicat-ingestor","title":"Infrastructure around Scicat Ingestor","text":"<p><code>scicat-ingestor</code> is written for specific infrastructure setup like below:</p> <pre><code>---\ntitle: Infrastructure around Scicat Ingestor\n---\ngraph LR\n    filewriter@{ shape: processes, label: \"File Writers\" } -.write file.-&gt; storage[(Storage)]\n    filewriter --report (wrdn)--&gt; kafkabroker[Kafka Broker]\n    ingestor[Scicat Ingestor] -.subscribe (wrdn).-&gt; kafkabroker\n    storage -.read file.-&gt; ingestor\n    ingestor --report--&gt; log[Gray Log]\n\n</code></pre> Framework Required Description Scicat O Scicat service that <code>scicat ingestor</code> can ingest files to. Kafka O Kafka broker that <code>scicat ingestor</code> can receive <code>write done</code> messages from.All messages are assumed to be serializedas flatbuffer using these schema: flatbuffer schemas for filewriter<code>scicat-ingestor</code> uses python wrapper of those schemas to deserialize messages.Currently only <code>wrdn</code> schema is used. File Writer O and X Any process that can write files and produce <code>write done</code> messages can be used. GrayLog X - optional <code>scicat ingestor</code> has built in <code>stdout</code> logging option. <p></p>"},{"location":"#file-ingesting-sequence","title":"File Ingesting Sequence","text":"<p>Here is a simple overview of how the ingestion is done.</p> <pre><code>---\ntitle: File Ingesting Sequence\n---\n\nsequenceDiagram\n  create participant File Writer\n  create actor File\n  File Writer --&gt; File: File Written\n  loop Ingest Files\n    Ingestor --&gt;&gt; Kafka Broker: Subscribe(listening to writing done - wrdn)\n    Kafka Broker -&gt;&gt; Ingestor: Writing Done Message (wrdn)\n    Note over Ingestor: Parse writing done message\n    Ingestor -&gt;&gt; File: Check file\n    opt\n        Ingestor -&gt;&gt; File: Parse Metadata\n    end\n    Note over Ingestor: Wrap files and metadata as Scicat Dataset\n    critical\n        Ingestor -&gt;&gt; Scicat: Ingest File\n    end\n  end\n\n</code></pre> <p></p> <p>Here is the typical file writing sequence including when the files are created/open.</p>          Click to see the File Writing Sequence      <pre><code>sequenceDiagram\nloop File Writing\n    File Writer --&gt;&gt; Kafka Broker: Subscribe (run start)\n    Kafka Broker -&gt;&gt; File Writer: Run Start\n    create actor File\n    File Writer -&gt;&gt; File: Create File and Close\n    File Writer --&gt; File: Open File as Append Mode\n    loop File Writing\n        File Writer --&gt;&gt; Kafka Broker: Subscribe Relevant Topics for the run\n        Kafka Broker -&gt;&gt; File Writer: Detector Data/Log/etc ...\n        File Writer -&gt;&gt; File: Write Data in the File.\n    end\n    Kafka Broker -&gt;&gt; File Writer: Run Stop\n    File Writer --&gt; File: Close File.\n    Note over File Writer: Compose wrdn message including id and file path\nFile Writer -&gt;&gt; Kafka Broker: Report (writing done - wrdn)\nend\n\n</code></pre>"},{"location":"#used-at","title":"Used At","text":"<p><code>scicat ingestor</code> is mainly maintained by <code>ESS DMSC</code> but it can be used in any systems that have same infrastructure set up.</p>"},{"location":"#european-spallation-source","title":"European Spallation Source","text":""},{"location":"#quick-start","title":"Quick Start","text":"<p>We do not release <code>scicat-ingestor</code> into any package index services. You can directly download it from our github page.</p> <pre><code>git clone https://github.com/SciCatProject/scicat-ingestor.git\ncd scicat-ingestor\ngit fetch origin\ngit checkout v25.01.0  # Latest Version\npip install -e .  # It will allow you to use entry-points of the scripts,\n                  # defined in ``pyproject.toml``, under ``[project.scripts]`` section.\n</code></pre>"},{"location":"#contribution","title":"Contribution","text":"<p>Anyone is welcome to contribute to our project.</p> <p>Please check our <code>developer guide</code>.</p>"},{"location":"CODE_OF_CONDUCT/","title":"Contributor Covenant Code of Conduct","text":""},{"location":"CODE_OF_CONDUCT/#our-pledge","title":"Our Pledge","text":"<p>We as members, contributors, and leaders pledge to make participation in our community a harassment-free experience for everyone, regardless of age, body size, visible or invisible disability, ethnicity, sex characteristics, gender identity and expression, level of experience, education, socio-economic status, nationality, personal appearance, race, caste, color, religion, or sexual identity and orientation.</p> <p>We pledge to act and interact in ways that contribute to an open, welcoming, diverse, inclusive, and healthy community.</p>"},{"location":"CODE_OF_CONDUCT/#our-standards","title":"Our Standards","text":"<p>Examples of behavior that contributes to a positive environment for our community include:</p> <ul> <li>Demonstrating empathy and kindness toward other people</li> <li>Being respectful of differing opinions, viewpoints, and experiences</li> <li>Giving and gracefully accepting constructive feedback</li> <li>Accepting responsibility and apologizing to those affected by our mistakes,   and learning from the experience</li> <li>Focusing on what is best not just for us as individuals, but for the overall   community</li> </ul> <p>Examples of unacceptable behavior include:</p> <ul> <li>The use of sexualized language or imagery, and sexual attention or advances of   any kind</li> <li>Trolling, insulting or derogatory comments, and personal or political attacks</li> <li>Public or private harassment</li> <li>Publishing others' private information, such as a physical or email address,   without their explicit permission</li> <li>Other conduct which could reasonably be considered inappropriate in a   professional setting</li> </ul>"},{"location":"CODE_OF_CONDUCT/#enforcement-responsibilities","title":"Enforcement Responsibilities","text":"<p>Community leaders are responsible for clarifying and enforcing our standards of acceptable behavior and will take appropriate and fair corrective action in response to any behavior that they deem inappropriate, threatening, offensive, or harmful.</p> <p>Community leaders have the right and responsibility to remove, edit, or reject comments, commits, code, wiki edits, issues, and other contributions that are not aligned to this Code of Conduct, and will communicate reasons for moderation decisions when appropriate.</p>"},{"location":"CODE_OF_CONDUCT/#scope","title":"Scope","text":"<p>This Code of Conduct applies within all community spaces, and also applies when an individual is officially representing the community in public spaces. Examples of representing our community include using an official e-mail address, posting via an official social media account, or acting as an appointed representative at an online or offline event.</p>"},{"location":"CODE_OF_CONDUCT/#enforcement","title":"Enforcement","text":"<p>Instances of abusive, harassing, or otherwise unacceptable behavior may be reported to the community leaders responsible for enforcement at scipp[at]ess.eu. All complaints will be reviewed and investigated promptly and fairly.</p> <p>All community leaders are obligated to respect the privacy and security of the reporter of any incident.</p>"},{"location":"CODE_OF_CONDUCT/#enforcement-guidelines","title":"Enforcement Guidelines","text":"<p>Community leaders will follow these Community Impact Guidelines in determining the consequences for any action they deem in violation of this Code of Conduct:</p>"},{"location":"CODE_OF_CONDUCT/#1-correction","title":"1. Correction","text":"<p>Community Impact: Use of inappropriate language or other behavior deemed unprofessional or unwelcome in the community.</p> <p>Consequence: A private, written warning from community leaders, providing clarity around the nature of the violation and an explanation of why the behavior was inappropriate. A public apology may be requested.</p>"},{"location":"CODE_OF_CONDUCT/#2-warning","title":"2. Warning","text":"<p>Community Impact: A violation through a single incident or series of actions.</p> <p>Consequence: A warning with consequences for continued behavior. No interaction with the people involved, including unsolicited interaction with those enforcing the Code of Conduct, for a specified period of time. This includes avoiding interactions in community spaces as well as external channels like social media. Violating these terms may lead to a temporary or permanent ban.</p>"},{"location":"CODE_OF_CONDUCT/#3-temporary-ban","title":"3. Temporary Ban","text":"<p>Community Impact: A serious violation of community standards, including sustained inappropriate behavior.</p> <p>Consequence: A temporary ban from any sort of interaction or public communication with the community for a specified period of time. No public or private interaction with the people involved, including unsolicited interaction with those enforcing the Code of Conduct, is allowed during this period. Violating these terms may lead to a permanent ban.</p>"},{"location":"CODE_OF_CONDUCT/#4-permanent-ban","title":"4. Permanent Ban","text":"<p>Community Impact: Demonstrating a pattern of violation of community standards, including sustained inappropriate behavior, harassment of an individual, or aggression toward or disparagement of classes of individuals.</p> <p>Consequence: A permanent ban from any sort of public interaction within the community.</p>"},{"location":"CODE_OF_CONDUCT/#attribution","title":"Attribution","text":"<p>This Code of Conduct is adapted from the Contributor Covenant, version 2.1, available at https://www.contributor-covenant.org/version/2/1/code_of_conduct.html.</p> <p>Community Impact Guidelines were inspired by Mozilla's code of conduct enforcement ladder.</p> <p>For answers to common questions about this code of conduct, see the FAQ at https://www.contributor-covenant.org/faq. Translations are available at https://www.contributor-covenant.org/translations.</p>"},{"location":"getting-started/","title":"Getting Started","text":""},{"location":"getting-started/#how-to-install","title":"How to INSTALL","text":""},{"location":"getting-started/#released-version","title":"Released Version","text":"<pre><code>git clone https://github.com/SciCatProject/scicat-ingestor.git\ncd scicat-ingestor\ngit fetch --tags\ngit checkout $VERSION\npip install -e .  # It will allow you to use entry-points of the scripts,\n                  # defined in ``pyproject.toml``, under ``[project.scripts]`` section.\n</code></pre>"},{"location":"getting-started/#latestdevelopment-version","title":"Latest(Development) Version","text":"<pre><code>git clone https://github.com/SciCatProject/scicat-ingestor.git\ncd scicat-ingestor\npip install -e .  # It will allow you to use entry-points of the scripts,\n                  # defined in ``pyproject.toml``, under ``[project.scripts]`` section.\n</code></pre>"},{"location":"getting-started/#configuration","title":"Configuration","text":"<p>You can use a json file to configure options. There is a template, <code>resources/config.sample.yml</code> you can copy/paste to make your own configuration file.</p> <p>Tip</p> <p>Once you write your configuration file, you can validate it before using it to start an ingestor. <pre><code># It runs `validate_config_file` function in `scicat_configuration` module.\nscicat_validate_ingestor_config path/to/the/config/file.yml\n</code></pre></p> <p>It tries building nested configuration dataclasses from the configuration file. It will throw errors if configuration is invalid.</p> <p>Note</p> <p><code>Ingestor</code> ignores extra keywords that do not match the configuration dataclass arguments but <code>validator</code> throws an error if there are extra keywords that do not match the arguments.</p> <p>See configuration page for more detailed explanation what each option does.</p> <p>See configuration developer-guide page if you need to add/update any configurations for ingestor.</p>"},{"location":"getting-started/#how-to-run","title":"How to RUN","text":"<p>All commands have prefix of <code>scicat</code> so that you can use auto-complete in a terminal.</p> <p>Each command is connected to a free function in a module. It is defined in <code>pyproject.toml</code>, under <code>[project.scripts]</code> section.</p> <p>All scripts parse the system arguments and configuration in the same way.</p>"},{"location":"getting-started/#online-ingestor-highest-level-interface","title":"Online ingestor (Highest level interface)","text":"<p>You can start the ingestor daemon with certain configurations.</p> <p>It will continuously process <code>wrdn</code> messages and ingest the corresponding nexus files.</p> <pre><code>scicat_ingestor --logging.verbose -c PATH_TO_CONFIGURATION_FILE.yaml\n</code></pre> <p>A topic can contain non-<code>wrdn</code> message so the ingestor filters messages and ignores irrelevant types of messages.</p> <p>See configuration for how to use configuration files.</p>"},{"location":"getting-started/#background-ingestor-lower-level-interface","title":"Background ingestor  (Lower level interface)","text":"<p>You can also run the ingestor file by file.</p> <p>You need to know the path to the nexus file you want to ingest and also the path to the <code>done_writing_message_file</code> as a json file.</p> <pre><code>scicat_background_ingestor \\\n    --logging.verbose \\\n    -c PATH_TO_CONFIGURATION_FILE.yaml \\\n    --nexus-file PATH_TO_THE_NEXUS_FILE.nxs \\\n    --done-writing-message-file PATH_TO_THE_MESSAGE_FILE.yml\n</code></pre>"},{"location":"getting-started/#dry-run","title":"Dry run","text":"<p>You can add <code>--ingestion.dry-run</code> flag for dry-run testings.</p> <pre><code>scicat_ingestor --logging.verbose -c PATH_TO_CONFIGURATION_FILE.yaml --ingestion.dry-run\n</code></pre> <pre><code>scicat_background_ingestor \\\n    --logging.verbose \\\n    -c PATH_TO_CONFIGURATION_FILE.yaml \\\n    --nexus-file PATH_TO_THE_NEXUS_FILE.nxs \\\n    --done-writing-message-file PATH_TO_THE_MESSAGE_FILE.yml \\\n    --ingestion.dry-run\n</code></pre>"},{"location":"_mermaid_charts/_file_ingestion_sequence/","title":"file ingestion sequence","text":"<pre><code>---\ntitle: File Ingesting Sequence\n---\n\nsequenceDiagram\n  create participant File Writer\n  create actor File\n  File Writer --&gt; File: File Written\n  loop Ingest Files\n    Ingestor --&gt;&gt; Kafka Broker: Subscribe(listening to writing done - wrdn)\n    Kafka Broker -&gt;&gt; Ingestor: Writing Done Message (wrdn)\n    Note over Ingestor: Parse writing done message\n    Ingestor -&gt;&gt; File: Check file\n    opt\n        Ingestor -&gt;&gt; File: Parse Metadata\n    end\n    Note over Ingestor: Wrap files and metadata as Scicat Dataset\n    critical\n        Ingestor -&gt;&gt; Scicat: Ingest File\n    end\n  end\n\n</code></pre>"},{"location":"_mermaid_charts/_file_writer_sequence/","title":"file writer sequence","text":"<pre><code>sequenceDiagram\nloop File Writing\n    File Writer --&gt;&gt; Kafka Broker: Subscribe (run start)\n    Kafka Broker -&gt;&gt; File Writer: Run Start\n    create actor File\n    File Writer -&gt;&gt; File: Create File and Close\n    File Writer --&gt; File: Open File as Append Mode\n    loop File Writing\n        File Writer --&gt;&gt; Kafka Broker: Subscribe Relevant Topics for the run\n        Kafka Broker -&gt;&gt; File Writer: Detector Data/Log/etc ...\n        File Writer -&gt;&gt; File: Write Data in the File.\n    end\n    Kafka Broker -&gt;&gt; File Writer: Run Stop\n    File Writer --&gt; File: Close File.\n    Note over File Writer: Compose wrdn message including id and file path\nFile Writer -&gt;&gt; Kafka Broker: Report (writing done - wrdn)\nend\n\n</code></pre>"},{"location":"_mermaid_charts/_infra_structure/","title":"infra structure","text":"<pre><code>---\ntitle: Infrastructure around Scicat Ingestor\n---\ngraph LR\n    filewriter@{ shape: processes, label: \"File Writers\" } -.write file.-&gt; storage[(Storage)]\n    filewriter --report (wrdn)--&gt; kafkabroker[Kafka Broker]\n    ingestor[Scicat Ingestor] -.subscribe (wrdn).-&gt; kafkabroker\n    storage -.read file.-&gt; ingestor\n    ingestor --report--&gt; log[Gray Log]\n\n</code></pre>"},{"location":"_mermaid_charts/_ingestor_flow_details/","title":"ingestor flow details","text":"<pre><code>---\ntitle: Ingestor Flow Chart - Detail\n---\nflowchart LR\n\n    subgraph online [Online Ingestor]\n        direction TB\n        connect-to-kafka[Connect to Kafka Cluster] --&gt; subscription[Subscribe to Instrument Topics]\n        subscription --&gt; wait[Wait for Next Messages]\n        wait --&gt; done{{Done Writing Message?}}\n        done --&gt; |No| wait\n        done --&gt; |Yes| max-process{{Maximum Offline Ingestor Running?}}\n        max-process --&gt; |Yes| wait-running@{ shape: delay , label: \"Wait for previous ingestors\"}\n        wait-running --&gt; max-process\n        max-process --&gt; |No| start@{shape: circle, label: \"Start Offline Ingestor\"}\n        start --&gt; wait\n    end\n\n    subgraph offline [Offline Ingestor]\n        direction TB\n        start-offline@{shape: circle, label: \"Start Offline Ingestor\"}\n        start-offline --&gt; load-schema[Load Schema]\n        load-schema --&gt; select[Select Schema]\n        select --&gt; open[Open Nexus File, Event Data]\n        open --&gt; variable[Define Variables]\n        variable --&gt; populate[Populate Local Dataset]\n        populate --&gt; create[Create Dataset on Scicat]\n        create --&gt; create-origdataset[Create OrigDataset on Scicat]\n        create-origdataset --&gt; stop@{shape: dbl-circ, label: \"Finish Offline Ingestor\"}\n\n    end\n\n    online --&gt; offline\n\n    style start fill:green,stroke-width:4px,opacity:0.5;\n    style start-offline fill:green,stroke-width:4px,opacity:0.5;\n\n</code></pre>"},{"location":"_mermaid_charts/_metadata_retrieval_flow/","title":"metadata retrieval flow","text":"<pre><code>---\ntitle: Metadata Retrieval Flow Chart\n---\nflowchart TB\n\n    subgraph matching [Schema Matching]\n        direction TB\n        load-schema[Load Schema FIles] --&gt; files@{shape: docs, label: \"Sorted Schema Files by order\"}\n        files --&gt; select[Next Schema File]\n        select --&gt; cur-schema@{shape: notch-rect, label: \"Current Schema File\"}\n        cur-schema --&gt; match{Match?*using selector*}\n        input-file@{shape: lean-l, label: \"Input Nexus File\"} --&gt; match\n        match --&gt; |Yes| matched\n        match --&gt; |No| select\n        select --&gt; |Nothing Matched| no-match@{shape: loop-limit, label: \"Fall back to the default schema definition\"}\n        matched --&gt; |Current Schema Definition| selected@{shape: notch-rect, label: \"Selected Schema\"}\n        no-match --&gt; |Default Schema Definition| selected\n    end\n\n    subgraph variables [Schema Variable Set]\n        direction TB\n        selected --&gt; variable-definitions@{shape: docs, label: \"Variable Definitions\"}\n        variable-definitions --&gt; build-variable-set[Build Variable Key-Value Pairs]\n        build-variable-set --&gt; local-variable-set@{shape: docs, label: \"Local Variable Set\"}\n    end\n\n    subgraph dataset [Metadata Dataset]\n        direction TB\n        selected --&gt; dataset-definitions@{shape: docs, label: \"Dataset Definitions\"}\n        local-variable-set --&gt; dto@{shape: docs, label: \"Dataset Transfer Object (DTO)\"}\n        dataset-definitions --&gt; dto\n    end\n\n</code></pre>"},{"location":"developer-guide/adrs/","title":"Architecture Decision Records","text":"<p>Here we keep records of important software architecture decisions and the reasonabouts.</p>"},{"location":"developer-guide/adrs/#adr-000-decouple-continuous-discovery-process-and-individual-dataset-ingestion-process","title":"ADR-000: Decouple continuous discovery process and individual dataset ingestion process.","text":"<p><code>scicat ingestor</code> has two main responsibilities:     - Continuous discovery of a new dataset with related files     - Individual dataset ingestion from the discovery</p> <p>Previously (&lt;25.01.0) <code>scicat ingestor</code> was single process program that continuously processes messages and files in a loop. In other words, both responsibilities were deelpy coupled.</p> <p>As the <code>scicat ingestor</code> under went project wide refactoring,  we decided to decouple those responsibilities(functionalities) and extract individual dataset ingestion as an independent tool.</p>"},{"location":"developer-guide/adrs/#advantages","title":"Advantages","text":"<p>Here are some of advantages we discovered as we decoupled the discovery process and ingestion process.</p>"},{"location":"developer-guide/adrs/#smaller-tests","title":"Smaller Tests","text":"<p>A single program, as it was initially, was hard to test and also to maintain. For example, we had to send <code>kafka</code> message to trigger the ingestion and make the ingestor parse <code>metadata</code> from files just to test if it can ingest file to <code>scicat</code> accordingly. If they are decoupled we can split this huge test into three smaller tests:     - <code>kafka</code> message processing     - <code>metadata</code> extraction     - <code>scicat</code> ingestion This decoupling helps to implement faster unittests/integration tests on a smaller scope.</p>"},{"location":"developer-guide/adrs/#smaller-usage","title":"Smaller Usage","text":"<p>As the dataset ingestion is now an completely independently tool from the discovery process, we can easily run ingestion multiple times in case of error.</p>"},{"location":"developer-guide/adrs/#multi-processes-with-central-control","title":"Multi Processes with Central Control","text":"<p>Discovery process(online ingestor) spawns a sub process to start the ingestion process and continue listening to the next dataset. In reality, <code>online ingestor</code> spawns multiple processes to start <code>offline ingestor</code> as it could take a few seconds or even a few minutes depending on the metadata it needs to compute. Even if one of processes fails due to faulty metadata or unexpected structure of dataset, it will not affect the rest of healthy files and ingestions as it is on a separate process.</p>"},{"location":"developer-guide/adrs/#less-configurations","title":"Less Configurations","text":"<p>As the ingestion process(<code>offline ingestor</code>) now do not communicate with kafka anymore, it can use subset of <code>online ingestor</code> configurations, which makes it easier to go through the list of the configurations dedicated to <code>offline ingestor</code>.</p>"},{"location":"developer-guide/adrs/#easier-maintenance","title":"Easier Maintenance","text":"<p>Due to all advantages we mentioned above, the maintenance cost considerably reduced. It takes less for testing, hence less time to release the software.</p>"},{"location":"developer-guide/adrs/#configuration","title":"Configuration","text":"<p><code>online-ingestor</code> runs the <code>offline-ingestor</code> so the configuration of <code>offline-ingestor</code> can be a subset of the configuration of <code>online-ingestor</code>.</p> <p>Configuration(online) \\subset Configuration(offline)</p> <p>However, instead of passing individual configurations to the offline ingestor via command arguments, we simply pass the whole configuration file that <code>online-ingestor</code> was called with, to the <code>offline-ingestor</code> as it is much easier.</p> <p>Because it is not so practical to maintain two different json files when they are almost identica. Therefore we compromised to keep only one json file, which has all configuration options both for <code>online-ingestor</code> and <code>offline-ingestor</code>.</p> <p>So the configuration file became union set of two different configurations.</p> <p>Configuration = Configuration(online) \\cup Configuration(offline)</p>"},{"location":"developer-guide/adrs/#visualization-of-architecture","title":"Visualization of Architecture","text":"<p>Note</p> <p>These diagrams might be updated and be different from the first design.</p>"},{"location":"developer-guide/adrs/#ingestor-flow-chart-bird-eye-view","title":"Ingestor Flow Chart - Bird Eye View","text":""},{"location":"developer-guide/adrs/#ingestor-flow-chart-detail","title":"Ingestor Flow Chart - Detail","text":"<pre><code>---\ntitle: Ingestor Flow Chart - Detail\n---\nflowchart LR\n\n    subgraph online [Online Ingestor]\n        direction TB\n        connect-to-kafka[Connect to Kafka Cluster] --&gt; subscription[Subscribe to Instrument Topics]\n        subscription --&gt; wait[Wait for Next Messages]\n        wait --&gt; done{{Done Writing Message?}}\n        done --&gt; |No| wait\n        done --&gt; |Yes| max-process{{Maximum Offline Ingestor Running?}}\n        max-process --&gt; |Yes| wait-running@{ shape: delay , label: \"Wait for previous ingestors\"}\n        wait-running --&gt; max-process\n        max-process --&gt; |No| start@{shape: circle, label: \"Start Offline Ingestor\"}\n        start --&gt; wait\n    end\n\n    subgraph offline [Offline Ingestor]\n        direction TB\n        start-offline@{shape: circle, label: \"Start Offline Ingestor\"}\n        start-offline --&gt; load-schema[Load Schema]\n        load-schema --&gt; select[Select Schema]\n        select --&gt; open[Open Nexus File, Event Data]\n        open --&gt; variable[Define Variables]\n        variable --&gt; populate[Populate Local Dataset]\n        populate --&gt; create[Create Dataset on Scicat]\n        create --&gt; create-origdataset[Create OrigDataset on Scicat]\n        create-origdataset --&gt; stop@{shape: dbl-circ, label: \"Finish Offline Ingestor\"}\n\n    end\n\n    online --&gt; offline\n\n    style start fill:green,stroke-width:4px,opacity:0.5;\n    style start-offline fill:green,stroke-width:4px,opacity:0.5;\n\n</code></pre>"},{"location":"developer-guide/adrs/#adr-001-use-dataclass-instead-of-jinja-or-dict-to-create-datasetdata-block-instances","title":"ADR-001: Use <code>dataclass</code> instead of <code>jinja</code> or <code>dict</code> to create dataset/data-block instances.","text":"<p>We need a dict-like template to create dataset/data-block instances via scicat APIs.</p>"},{"location":"developer-guide/adrs/#reason-for-not-using-dict","title":"Reason for not using <code>dict</code>","text":"<p>It used to be implemented with <code>dict</code> but it didn't have any verifying layer so anyone could easily break the instances without noticing or causing errors in the upstream layers.</p>"},{"location":"developer-guide/adrs/#reason-for-not-using-jinja","title":"Reason for not using <code>jinja</code>","text":"<p><code>Jinja</code> template could handle a bit more complicated logic within the template, i.e. <code>for</code> loop or <code>if</code> statement could be applied to the variables. However, the dataset/data-block instances are not complicated to utilize these features of <code>jinja</code>.</p>"},{"location":"developer-guide/adrs/#reason-for-using-dataclassesdataclass","title":"Reason for using <code>dataclasses.dataclass</code>","text":"<p>First we did try using <code>jinja</code> but the dataset/data-block instances are simple enough so we replaced <code>jinja</code> template with <code>dataclass</code>. <code>dataclass</code> can verify name and type (if we use static checks) of each field. It can be easily turned into a nested dictionary using <code>dataclasses.asdict</code> function.</p>"},{"location":"developer-guide/adrs/#downside-of-using-dataclass-instead-of-jinja","title":"Downside of using <code>dataclass</code> instead of <code>jinja</code>","text":"<p>With <code>jinja</code> template, certain fields could be skipped based on a variable. However, it is not possible in the dataclass so it will need extra handling after turning it to a dictionary. For example, each datafile item can have <code>chk</code> field, but this field shouldn't exist if checksum was not derived. With jinja template we could handle this like below <pre><code>{\n    \"path\": \"{{ path }}\",\n    \"size\": {{ size }},\n    \"time\": \"{{ time }}\",\n    {% if chk %}\"chk\": \"{{ chk }}\"{% endif %}\n}\n</code></pre> However, with dataclass this should be handled like below. <pre><code>from dataclasses import dataclass, asdict\n@dataclass\nclass DataFileItem:\n    path: str\n    size: int\n    time: str\n    chk: None | str = None\n\ndata_file_item = {\n    k: v\n    for k, v in asdict(DataFileItem('./', 1, '00:00')).items()\n    if (k!='chk' or v is not None)\n}\n</code></pre></p> <p>Warning</p> <p> No Multi-type Arguments  We decided NOT to support multiple types of arguments/configuration option due to this ADR. It is not impossible to support it, but the advantange is not worth the effort of handling multiple types. Especially it makes the auto generator of json configuration file much more difficult than how it is now.</p> <p>For example, there can be multiple kafka brokers so in principle we could allow a list of string as an argument type or a single string value. However we decided to keep it only as a string, and if multiple brokers are needed, user should write them joined by comma(,). On the other hand, access group option is always <code>list</code> even if there may be only one access group.</p>"},{"location":"developer-guide/adrs/#adr-002-use-yaml-instead-of-json-for-metadata-schema-and-ingestor-configuration","title":"ADR-002: Use <code>yaml</code> instead of <code>json</code> for metadata schema and ingestor configuration.","text":"<p>Use <code>yaml</code> for human-interacting configuration files and <code>json</code> for communication between processes or services.</p>"},{"location":"developer-guide/adrs/#reason","title":"Reason","text":"<p><code>Yaml</code> has better human readability compared to <code>json</code> and allows <code>comments</code>. It will be much easier with commenting allowed to share configuration details and context with other maintainers.</p> <p>We decided to make schema files modular. It means service maintainers can build complicated metadata schema without duplicating schema files. That means it should be easy to track intention/context of each module. So <code>comments</code> feature of configuration file was prioritized to the robustness/performance of parsing the configuration.</p>"},{"location":"developer-guide/adrs/#downside","title":"Downside","text":"<p>Parsing <code>Yaml</code> can be much slower than <code>json</code> if we have too complex structure. However, we will avoid such structure of configuration as it is mainly written by human.</p> <p>Another downside is that <code>Yaml</code> is less secure than <code>json</code> due to its flexibility. Service maintainers are expected to keep these configuration/schema files in a secured isolated environment and we implement extra safety/validation layers of configuration/schema files in the relevant places. For example, <code>yaml</code> must be loaded with <code>safe_load</code> method.</p>"},{"location":"developer-guide/adrs/#supporting-tool-for-transition","title":"Supporting Tool for Transition","text":"<p>We were already using <code>json</code> in our production environment so to make the transition smoother, we implemented a tool that can translate <code>json</code> to <code>yaml</code> easily.</p> <pre><code>scicat_json_to_yaml --input-file PATH/TO/THE/JSON/FILE\n</code></pre> <p>It can be used for any configuration or schema files.</p>"},{"location":"developer-guide/configuration/","title":"Configuration Development","text":"<p>As <code>scicat-ingestor</code> communicates with various frameworks via network, there are many configurations to maintain and handle.</p> <p>Therefore we decided to group them as a nested dataclass and keep the dataclass as a source of truth. Then it is exported as a json file in <code>resources/config.sample.yml</code> which has all possible options with their default values.</p> <p>See Architecture Decision Records to see why.</p> <p>Argument parser is also automatically built by <code>scicat_configuration.build_arg_parser</code>.</p>"},{"location":"developer-guide/configuration/#update-configuration","title":"Update Configuration","text":"<p>If you need to change anything in the configuration, you will have to update data classes and sync the dataclasses with the <code>json</code> config file.</p> <p>There should be one entry configuration that contains all configurations for the scicat ingestor program. <code>online ingestor</code> and <code>offline ingestor</code> use different configuration class.</p> online ingestor offline ingestor OnlineIngestorConfig OfflineIngestorConfig <p>Once any configuration is updated, it should be exported to json using this command that comes with the package.</p> <pre><code># It calls `synchronize_config_file` function in `scicat_configuration` module.\nscicat_synchronize_config\n</code></pre> <p>There is also a unit test to check if they are in sync so don't worry about forgetting them. CI will scream about it...!</p>"},{"location":"developer-guide/configuration/#using-configuration","title":"Using Configuration","text":"<p>Any other modules that needs configurations should always use the configuration dataclass object, instead of a plain dictionary.</p> <p>Tip</p> <p>Sometimes, it is easier to have a <code>read-only</code> property in configuration dataclass object that is built as it is read based on the other configuration properties.</p> <p>For example:</p> <pre><code>@dataclass(kw_only=True)\nclass SciCatOptions:\n    host: str = \"https://scicat.host\"\n    token: str = \"JWT_TOKEN\"\n    additional_headers: dict = field(default_factory=dict)\n    ...\n\n    @property\n    def headers(self) -&gt; dict:\n        return {\n            **self.additional_headers,\n            **{\"Authorization\": f\"Bearer {self.token}\"},\n        }\n\n    ...\n</code></pre> <p>And communication module can simply access to <code>scicat_options.hearders</code> instead of building the header itself.</p>"},{"location":"developer-guide/configuration/#argument-parser","title":"Argument Parser","text":"<p>As the argument parser is automatically built, there are some manual argument-configuration registries in <code>scicat_configuration.py</code>.</p>"},{"location":"developer-guide/configuration/#helper-text-registry","title":"Helper Text Registry","text":"<p><code>scicat_configuration._HELP_TEXT</code></p> Why though?     Unfortunately dataclass properties docstring cannot be parsed dynamically.     Therefore we made this registry to add useful help text to certain arguments. <p><code>_HELP_TEXT</code> mapping proxy that holds all mappings from the long name to custom help-text. The keys should be the long name of the argument including its group without <code>--</code>.</p> <p>For example, if you want to add helper text to <code>dry-run</code>, you have to add  <code>\"ingestion.dry-run\": \"Dry run mode. No data will be sent to SciCat.\"</code> to the registry.</p>"},{"location":"developer-guide/configuration/#short-name-registry","title":"Short Name Registry","text":"<p><code>scicat_configuration._SHORTENED_ARG_NAMES</code></p> Why though?     Most of arguments will be passed to the ingestor from `config file`.     However `config-file` option can't be passed from the `config file` (obviously).     To make it more convenient to start the ingestor we wanted to give it a short name.  <p><code>_SHORTENED_ARG_NAMES</code> mapping proxy that holds all mappings from the long name to short name. The keys should be the long name of the argument including its group without <code>--</code>.</p> <p>For example, if you want to use <code>d</code> for <code>dry run</code> configuration, you have to add  <code>\"ingestion.dry-run\": \"d\"</code> to the registry.</p>"},{"location":"developer-guide/documentation/","title":"Documentation","text":"<p>How to build and update documentation.</p> <p>We use <code>mkdocs</code> to build documentation.</p>"},{"location":"developer-guide/documentation/#build-locally","title":"Build Locally","text":"<p><code>tox -e docs</code> can build documentation locally but you can also use <code>mkdocs</code> command to serve/build the documentation.</p> <ul> <li><code>mkdocs serve</code> - Start the live-reloading docs server.</li> <li><code>mkdocs build</code> - Build the documentation site.</li> </ul> <p>Note</p> <p>CI action uses <code>mike</code> instead of using <code>mkdocs</code> directly to build and deploy the documentation site at the same time.</p>"},{"location":"developer-guide/getting-started/","title":"Getting Started - Development","text":""},{"location":"developer-guide/getting-started/#tldr","title":"TL;DR","text":"<pre><code>git clone git@github.com:SciCatProject/scicat-ingestor.git\nconda create -n scicat-ingestor-dev python=3.12\nconda activate scicat-ingestor-dev\npip install -r requirements/dev.txt\npre-commit install\npip install -e .\n</code></pre>"},{"location":"developer-guide/getting-started/#git","title":"Git","text":"<p>If you are not <code>scicat-ingestor</code> maintainors,  you need to either fork the repository to your own organization of private account and create a PR from there.</p> <pre><code>git clone git@github.com:SciCatProject/scicat-ingestor.git\n</code></pre>"},{"location":"developer-guide/getting-started/#virtual-environment","title":"Virtual Environment","text":"<p><code>scicat-ingestor</code> is a python project. We make multiple lock files for various environments and dev tools.</p> <p><code>requirements/dev.txt</code> contains all dependencies for development and tools.</p> <pre><code>conda create -n scicat-ingestor-dev python=3.12  # One and only supported version by scicat ingestor.\nconda activate scicat-ingestor-dev\npip install -r requirements/dev.txt\npip install -e .\n</code></pre> <p>The rest of the instruction will assume that this virtual environment is activated.</p>"},{"location":"developer-guide/getting-started/#pre-commit-hook","title":"Pre-commit Hook","text":"<pre><code>pre-commit install\n</code></pre> <p>Note</p> <p>Pre commit hooks are configured in <code>.pre-commit-config.yaml</code> file but some of <code>ruff</code> configurations are in <code>pyproject.toml</code>.</p>"},{"location":"developer-guide/getting-started/#testings","title":"Testings","text":"<p>We have unit tests using <code>pytest</code> that can be run fast and often.</p> <p><code>tox -e py312</code> command will run pytest with pre-defined configuration in a virtual environment created with <code>requirements/test.txt</code>.</p> <p>There are also integration test in github ci action.</p>"},{"location":"developer-guide/getting-started/#other-devops-routines","title":"Other DevOps Routines","text":""},{"location":"developer-guide/getting-started/#copier-update","title":"Copier Update","text":"<p>Copier template is already set up in <code>.copier-answers.yml</code> so you just need to update it once in a while.</p> <pre><code>copier update\n</code></pre> <p>It will ask a lot of questions and most of them usually stay the same.</p> <p>Here are some properties of the project that should be updated by copier:</p> <pre><code>- python version\n- project name\n- CI actions\n- requirements/make_base.py file (Please report any bugs in this file to the template repository.)\n</code></pre>"},{"location":"developer-guide/getting-started/#lock-dependencies","title":"Lock Dependencies","text":"<p><pre><code>tox -e deps\n</code></pre> This command will compile all <code>*.in</code> files and create corresponding <code>*.txt</code> lock files under <code>requirements</code>.</p> <p>Once you create the lock files, push it to the project into a separate branch and create a PR to main branch.</p> <p>Base dependencies are parsed from <code>pyproject.toml</code> project dependencies and written into <code>base.in</code> file.</p> <p>See <code>testenv:deps</code> section in <code>tox.ini</code> file to see what it does.</p>"},{"location":"developer-guide/getting-started/#dev-toolscommands-overview","title":"Dev Tools/Commands Overview","text":"Tool/Command Configuration File Description pre-commit .pre-commit-config.yaml Pre-commit hooks including linter checks.Once it's set up, it will be run automatically whenever a new commit is created. It is also run by one of CI actions.Bypass Pre Commit CheckYou can skip pre-commit checks with <code>--no-verify</code> flag: <code>git commit --no-verify</code>. But please keep it passing as much as possible, as it is one of blocking CI tests. copier .copier-answers.yaml This project copies from <code>scipp copier template</code>You have to manually update from copier template once in a while.See Copier Update for more explanation. tox tox.ini Multiple tox environment/commands for development and CI actions.It creates virtual environments and use it for each commands.The virtual environment files are saved under <code>.tox</code> directory. <code>tox -e deps</code> tox.ini/[testenv:deps] Create lock files with dependencies. <code>tox -e static</code> tox.ini/[testenv:static] Run all precommit hooks on all files. <code>tox -e py312</code> tox.ini/[testenv] Run pytests with python version 3.12.You can pass more arguments to pytest. <code>tox -e mypy</code> tox.ini/[testenv:mypy] Run static type checks with <code>mypy</code>. <code>tox -e docs</code> tox.ini/[testenv:docs] Build documentation site. This part of the copier template is overwritten since this project does not require sphinx."},{"location":"developer-guide/release-deploy/","title":"Release and Deployment","text":"<p>Currently the <code>scicat-ingestor</code> is not deployed to any package indexing services, like <code>pypi</code> or <code>anaconda</code>. It is expected to be installed from the source code, which is available from the github repository.</p>"},{"location":"developer-guide/release-deploy/#tldr-releasing-scicat-ingestor","title":"TL;DR - Releasing <code>scicat-ingestor</code>","text":"<ul> <li>Publish and tag the main branch with the version number.</li> <li>Deploy new documentation from the release.</li> </ul>"},{"location":"developer-guide/release-deploy/#release-steps","title":"Release Steps","text":"<p>Here is instruction of release steps in detail.</p>"},{"location":"developer-guide/release-deploy/#1-go-to-github-repository-releases-page","title":"1. Go to github repository releases page","text":"<p>Scicat Intestor Releases Page</p>"},{"location":"developer-guide/release-deploy/#2-click-draft-a-new-release-button","title":"2. Click <code>Draft a new release</code> button","text":""},{"location":"developer-guide/release-deploy/#3-create-a-new-tag-version-number","title":"3. Create a new tag (version number)","text":"<p>This will be the title of the release as well.</p> <p>Version Format: <code>YY.MM.x</code></p> <ul> <li><code>YY</code>: Last two digits of the current year</li> <li><code>MM</code>: Current month in two digits</li> <li><code>x</code>: Patch version number of the current month.         If it is the first version of this month, it should be <code>0</code>         and if it is the second version of the month, it should be <code>1</code> and so on...</li> </ul> <p></p>"},{"location":"developer-guide/release-deploy/#4-select-previous-tag","title":"4. Select previous tag","text":"<p>Select the previous tag to compare the current version.</p> <p>Note</p> <p>Finding previous tag automatically by setting it to <code>auto</code> does not work currently.</p> <p></p>"},{"location":"developer-guide/release-deploy/#5-generate-release-notes","title":"5. Generate release notes","text":"<p>Click the <code>Generate release notes</code> button to generate release notes.</p> <p>Then it creates release notes based on the PR merges.</p> <p></p>"},{"location":"developer-guide/release-deploy/#6-edit-release-notes","title":"6. Edit release notes","text":"<p><code>Generate release notes</code> button will create the release title and the notes based on the PRs that have been merged since the <code>previous tag</code> you selected.</p> <p>Under <code>What's Changed</code> notes, we can typically remove <code>Bump</code> PRs by <code>dependabot</code> or <code>copier update</code>.</p> <p>You can add more notes if needed.</p> <p></p>"},{"location":"developer-guide/release-deploy/#7-publish-release","title":"7. Publish release","text":"<p>Click the <code>Publish release</code> button to publish the release.</p> <p></p>"},{"location":"developer-guide/release-deploy/#8-check-the-documentation","title":"8. Check the documentation","text":"<p>Publishing the release should trigger the documentation build/deploy action.</p> <p>Note</p> <p>This action might fail due to various reasons so please check if it is deployed correctly by checking the action status or the documentation url.</p> <p>Warning</p> <p>This release action is not directly connected to the <code>scicat-ingestor</code> deployment at the ESS infrastructure.</p> <p>There is a separate CI/CD pipeline that pulls the latest release of <code>scicat-ingestor</code>.</p>"},{"location":"developer-guide/release-deploy/#9-new-release-of-the-scicat-ingestor","title":"9. \ud83c\udf89 New release of the <code>scicat-ingestor</code> \ud83c\udf89","text":"<p>The new release will look similar to this:</p> <p></p>"},{"location":"user-guide/configuration/","title":"Configuration","text":"<p>As <code>scicat-ingestor</code> communicate to multiple frameworks, it has many configurable options.</p> <p>Therefore configuration is divided into several sections.</p> <p>Note</p> <p>Both online and offline ingestor share the same configuration file which can be passed in with <code>-c</code> options.</p> <p>TODO: Turn these sections into a table.</p>"},{"location":"user-guide/configuration/#general","title":"General","text":"<p>This section contains few fields that are used to configure the input parameters of the ingestor programs. Most of the time they are left empty as they are specified on the command line. It also include the id of the configuration which is an arbitrary string, but for easy tracking and maintenance we suggest to be unique within your system. We also suggest to use uuids and change every time the configuration is updated.</p> <ul> <li>nexus_file   This is the data file that the ingestor needs to ingest and should be contained in the dataset to be created.   At the moment, the ingestor can handle only nexus files. If there is the need, additional formats can be added.   The file to be ingested can be specified here, but in most cases, this option is left empty and the data file is passed as an argument at the command line.</li> <li>done_writing_message_file   This is the message received by the online ingestor, if it is configured to save the message content to a file.   At the moment, the naming is specific to ESS. It will be changed if the tool is adopted by other facilities.   This option is also left empty most of the time as the file name is passed as command line option, if present.</li> <li>config_file   This is the location of the configuraition file where all this options are saved.   It is always empty as such file is provided as an argument with the option <code>-c</code>.</li> <li>id   This is a string which should be set to a unique value for the admin convenience.   We suggest to use uuids but there are no restrictions applied to it. We also suggest to change the id every time the config is changed, so it is easy to debug and troubleshoot where automatic deployment system are in use.   Example: \"c1eef5a0-9b8b-11ef-a991-0bfe16577c30\",</li> </ul>"},{"location":"user-guide/configuration/#dataset","title":"Dataset","text":"<p>This section pertains to the offline ingestor and contains all the options related to creating the dataset in scicat. It still contains some legacy options that are not in use anymore. They will be marked as obsolete and remove in the near future.</p> <ul> <li>allow_dataset_pid: boolean   This option instruct the ingestor if can specify the dataset pid or it should rely on SciCat to assign it the newly created dataset.   Options:</li> <li>False: SciCat will assign the PID</li> <li>True: The ingestor assigns the PID</li> <li>generate_dataset_pid: boolean   This option instruct the ingestor to generate a random pid for the dataset. The generate pid will be a uuid4 string.   Options:</li> <li>False: the ingestor will not generate a random PID</li> <li>True: the ingestor will generate a random PID</li> <li>dataset_pid_prefix: string   This option provides the facility specific prefix for the dataset uuids.   Example: \"20.500.12269\",</li> <li>default_instrument_id: string   This option specify which is the default instrument that the dataset will be associated if the instrument is not provided or is invalid.   Example: \"20.500.12269/00fe23a2-f276-4e2e-9005-a89a9c6ae9fe\"</li> <li>default_proposal_id: string   This option specify which is the default proposal id to be used if not provided or the one provided is invalid.   Example: \"070910\"</li> <li>default_owner_group: string   This option specify which is the owner group of the dataset if it is not provided.   Example: \"070910\"</li> <li>default_access_groups: string[]   This options provides a list of string that should be used as access groups if not provided.   Example: [ \"group_1\", \"group_2\" ]</li> </ul>"},{"location":"user-guide/configuration/#ingestion","title":"Ingestion","text":"<p>This section is specific for the offline ingestor and contains all the options to configure the ingestion process, such as path to the offline ingestor, where are the schema files, whhich tests to do before creation and which files to create.</p> <ul> <li>dry_run: boolean   This option instruct the ingestor to run in dry run mode. It performs all the required actions with the associated logs, except the ones that modify the data catalog.   Options:</li> <li>false: perform the ingestions</li> <li>true: does all the preparation, but it does not attempt to run offline ingestor (if running th eonline ingestor), or create a dataset entry in SciCat (if running the offline ingestor).</li> <li>offline_ingestor_executable: string[]   This option provides the online ingestor with the command line needed to run the offline ingestor. It is an array of strings as the ingestor is a python scripts and the admin might want to specify a specific version of python that is not the default one.   Example: [ \"/root/micromamba/envs/scicat-ingestor/bin/python\", /ess/services/scicat-ingestor/software/src/scicat_offline_ingestor.py\" ]</li> <li>schemas_directory: string   This option define the full path of the folder where all the imsc files are located in this installation. It is used in the offline ingestor only. Please visit the section relative tpo the schema for more information about the imsc files.   Example: \"/ess/services/scicat-ingestor/schemas\",</li> <li>check_if_dataset_exists_by_pid:boolean   This option instruct the offline ingestor to check if a dataset with the same pid already exists in the scicat instance of reference.   Options:</li> <li>false: do no check and attempt directly creating the dataset</li> <li>true: check if a dataset with the same pid already exists and do not create the dataset if already present</li> <li>check_if_dataset_exists_by_metadata: boolean   this option instruct the offline ingestor to check if a dataset with a specific metadata value is already present in the scicat instance of reference.   Options:</li> <li>false: do not check and attempt directly creating the dataset</li> <li>true: check on the specified metadata key if a dataset already exists and do not create the dataset if already present</li> <li>check_if_dataset_exists_by_metadata_key: string   Name of the metadata field that we need to check for dataset exists if option check_if_dataset_exists_by_metadata   Example: \"job_id\"</li> </ul>"},{"location":"user-guide/configuration/#file-handling-file_handling","title":"File Handling (file_handling)","text":"<ul> <li>compute_file_stats: boolean   This option instruct the offline ingestor to include in the scicat dataset file list the os statistics (aka ownership, permissions and size).   Options:</li> <li>false: no stats are added in the SciCat record for this file</li> <li> <p>true: stats are added in the SciCat record for this file   Example:</p> </li> <li> <p>compute_file_hash: boolean   This option instruct the offline ingestor to include in the scicat dataset file list the hash of the file content computed with the algorithm specified in option file_hash_algorithm.   Options:</p> </li> <li>false: do not compute the hash</li> <li>true: compute the hash and insert it in the scicat record for the file</li> <li>file_hash_algorithm: string   Selected algorithm used to compute the file hash.   Available options: blake2b   Example: \"blake2b\"</li> <li>save_file_hash: boolen   This options instruct the offline ingestor to save the computed hash also in a file located in the folder specified ingestor_files_directory in a file that is named with the original data file name with the suffix defined in option hash_file_extension   Options:</li> <li>false: do not save the hash file</li> <li>true: save the hash file in addition to store it in scicat</li> <li>hash_file_extension: string   Extension appended to the data file name to define the hash file name.   Example: \"b2b\"</li> <li>ingestor_files_directory: string   This option provides the full path of the folder where all the files produced by the ingestor process should saved. It needs to be a valid full path.   Example: \"/ess/services/scicat-ingestor/output\"</li> <li>message_to_file: boolean   This option instruct the online ingestor to save the notification message received through the selected medium in a file with name compose by the original file name and the suffix specified in option message_file_extension.   Options:</li> <li>false: do not save original notification message in a file</li> <li>true: save original notification message in a json file</li> <li>message_file_extension: string   Extension used for the message file if saved.   Example: \"message.json\"</li> <li>file_path_type: string   This option instruct the ingestor to use the specified type of path.   Available options:</li> <li>absolute: use absolute paths</li> <li>...</li> </ul>"},{"location":"user-guide/configuration/#kafka-kafka","title":"Kafka (kafka)","text":"<p>This section is used by the online ingestor and it configures how to connect to the kafka server and which topics to listen to, It also specify the parameters related to how to retrieve the messages. Please ask your kafka admin for more information.</p> <ul> <li>topics: string[]   This option provide to the online ingestor the list of the topics to listen on the kafka cluster for the message informing the availability of a new data file.   Example: [ \"ymir_filewriter\", \"coda_filewriter\" ]</li> <li>group_id: string   This option instruct the online ingestor to connect to the kafka cluster with the specified id. Multiple instances of the ingestor can use the same id. In this case the kafka cluster will distribute the messages across the instances running under the same group.   Example: \"scicat_filewriter_ingestor_05\"</li> <li>bootstrap_servers: string   This option provides the online ingestor with the comma separate list of the ip addresses of the kafka server that accept a connection including the port.   Example: \"10.100.4.15:8093,10.100.5.17:8093,10.100.5.29:8093\"</li> <li>security_protocol: string   This option specify the security protocol that the online ingestor should use to connect to the kafka cluster.   Example: \"sasl_ssl\"</li> <li>sasl_mechanism: string   This option provides to the online ingestor which sasl mechanism should be used to connect to the kafka cluster.   Example: \"SCRAM-SHA-256\",</li> <li>sasl_username: string   This options provides the user name use in the sasl protocol when connecting to the kafka cluster   Example: \"scicat_ingestor\"</li> <li>sasl_password: string   This field provides the password associated with the user name for the sasl protocol when connecting to the kafka cluster</li> <li>ssl_ca_location: string   This the full path to the file containing the ssl certificate needed to connect to the kafka cluster.   Example: \"/ess/services/scicat-ingestor/ssl/ecdc-kafka-ca.crt\"</li> <li>auto_offset_reset: string   This option specify the strategy on how to retrieve the messages from the kafka topic. Please consult kafka documentation for all the available options.   Example: \"earliest\",</li> <li>enable_auto_commit: boolean   This option instruct the online ingestor to auto commit all the messages.   Options:</li> <li>false: do not auto commit</li> <li>true: auto commit all the messages</li> <li>individual_message_commit: boolean   This option configure the online ingestor to commit each message individually. At the moment this option is unstable and its use is not recommended.   Options:</li> <li>false: do not auto commit each individual message</li> <li>true: commit each individual message searately.</li> </ul>"},{"location":"user-guide/configuration/#logging-logging","title":"Logging (logging)","text":"<p>This section is used by both programs and it specify how and where to send the logs. Logs can be sent to a file, the OS system and to a separate graylog. At the moment there are no integration with other log server, but they can be added as the need arise.</p> <ul> <li>verbose: boolean   Enable verbose logging for all the ingestor applications</li> <li>file_log: boolean   Enable logging to files. Each run is logged in a separate file with a name composed by the path specified in option file_log_base_name followed by the timestamp (if enabled) and the .log extension.</li> <li>file_log_base_name: string   This option provide the common full path for the log file name.   Example: \"/ess/services/scicat-ingestor/logs/scicat_ingestor_log\",</li> <li>file_log_timestamp: boolean   Enables the timestamp to be added in the log file name.</li> <li>logging_level: string   Level of the logging. Any logs that matches the level specified or higher is sent to the logging facility.   for more information about logging levels, please visit the following page: https://docs.python.org/3/library/logging.html#levels   Example: \"INFO\",</li> <li>log_message_prefix: string   Set the string that is affixed to every message logged.   Example: \"SCI\" or \"SciCat Ingestor\"</li> <li>system_log: _boolean   Enable logging to the system log. Please consult your OS logging documentation for more information where to find such logs.   The system logs have been tested on *nix systems only. On different OS, mileage may vary.</li> <li>system_log_facility: string   If running on *nix system, under which log system the messages are logged.   Example: \"mail\",</li> <li>graylog: boolean   Enable logging to a graylog system through the SciCat graylog integration.</li> <li>graylog_host: string   Ip address or valid hostname of the graylog server where the ingestor should send the logs   Examples: \"graylog.ess.eu\"</li> <li>graylog_port: integer as a string   Port on which the graylog service is listening and accepting remote logs on the graylog server   Example: \"12321\",</li> <li>graylog_facility: string   Arbitrary string used in the facility field for the graylog messages.   It is suggested to pick a meaningful string that can be used when selecting entries in graylog interface and queries   Example: \"scicat_ingestor\"</li> </ul>"},{"location":"user-guide/configuration/#scicat-scicat","title":"SciCat (scicat)","text":"<p>This section is used by the offline ingestor and it contains the info about url where the relevant scicat instance is reachable and also the token that needs to be used for authentication purposes.</p> <ul> <li>host: valid url as string   URL of the SciCat instance of reference where we want to create the dataset records.   Example: \"https://scicat.ess.eu/api/v3\",</li> <li>token: string   Valid JWT token used to connect to SciCat with the proper permissions to query and create datasets.</li> <li>timeout: integer or null   Value in ms for the timeout when placing requests to SciCat.</li> <li>stream: boolean   Obscure option for the request library when connecting to a URL and send requests.   Usually set to true.   Please refer to the python requests library documentation for more information.</li> <li>verify: boolean   Obscure option for the request library when connecting to a URL and send requests.   Usually set to false.   Please refer to the python requests library documentation for more information.</li> </ul>"},{"location":"user-guide/metadata-schemas/","title":"Metada Schemas","text":"<p><code>scicat-ingestor</code> parses or computes metadata from ingested dataset. <code>schema files</code> are a set of directives of how to collect/compute the metadata. The directives include <code>where(from which source - file, scicat or the schema itself, etc...)</code> to retrieve the information and <code>how</code> to compute/parse the information. Using the directives, <code>scicat-ingestor</code> populate the dataset entry in <code>SciCat</code>.</p> <p>In more detail, each schema file contains following information: |   |   | | - | - | | schema selection | When to apply the schema to the file | | variables definition | How to construct a <code>local variable set</code>, i.e. variable <code>pid</code> is the string from <code>/entry/entry_identifier_uuid</code> in the nexus file. | | dataset creation | How to populate fields in a SciCat dataset using the <code>local variable set</code> |</p> <p>The schema files are written in <code>yaml</code> format and all have <code>imsc.yml</code> extension. <code>imsc</code> stands for SciCat <code>i</code>ngestor <code>m</code>etadata <code>sc</code>hema. Each schema file has mandatory fields.</p>"},{"location":"user-guide/metadata-schemas/#content-structure","title":"Content Structure","text":"<p>An <code>imsc</code> file has three main sections, general, variables and schemas.</p> Section Description general A group of fields including <code>unique id</code>, <code>name of the schema</code>, <code>which instrument</code> it's associated withand the matching-criteria that tells which file is relevant to this schema.  See schema definition (general information) for more information. variables A set of <code>key-value</code> pairs retrieved from multiple sources.They are later used in the schema section.See variable set instruction for more information. schema A set of instructions how to populate <code>SciCat</code> dataset fields using <code>variables</code>.See SciCat dataset population instruction for more information."},{"location":"user-guide/metadata-schemas/#example","title":"Example","text":"<p>This is an example of schema file:</p> <pre><code>{\n  \"order\": \"&lt;NUMBER&gt;\",\n  \"id\": \"&lt;UNIQUE_STRING&gt;\",\n  \"name\" : \"&lt;NAME_OF_THE_SCHEMA&gt;\",\n  \"instrument\" : \"&lt;COMMA_SEPARATED_LIST_OF_INSTRUMENTS_NAMES&gt;\",\n  \"selector\" : \"&lt;OBJECT_WITH_SELECTION_RULES&gt;\",\n  \"variables\" : {\n    \"&lt;VARIABLE_NAME&gt;\" : \"&lt;VARIABLE_DEFINITION&gt;\"\n  },\n  \"schemas\" : {\n    \"&lt;MNEMONIC_ENTRY_NAME&gt;\" : \"&lt;SCHEMA_ENTRY_DEFINITION&gt;\"\n  }\n}\n</code></pre>"},{"location":"user-guide/metadata-schemas/#metadata-schema-retrieval-flow","title":"Metadata Schema Retrieval Flow","text":"<pre><code>---\ntitle: Metadata Retrieval Flow Chart\n---\nflowchart TB\n\n    subgraph matching [Schema Matching]\n        direction TB\n        load-schema[Load Schema FIles] --&gt; files@{shape: docs, label: \"Sorted Schema Files by order\"}\n        files --&gt; select[Next Schema File]\n        select --&gt; cur-schema@{shape: notch-rect, label: \"Current Schema File\"}\n        cur-schema --&gt; match{Match?*using selector*}\n        input-file@{shape: lean-l, label: \"Input Nexus File\"} --&gt; match\n        match --&gt; |Yes| matched\n        match --&gt; |No| select\n        select --&gt; |Nothing Matched| no-match@{shape: loop-limit, label: \"Fall back to the default schema definition\"}\n        matched --&gt; |Current Schema Definition| selected@{shape: notch-rect, label: \"Selected Schema\"}\n        no-match --&gt; |Default Schema Definition| selected\n    end\n\n    subgraph variables [Schema Variable Set]\n        direction TB\n        selected --&gt; variable-definitions@{shape: docs, label: \"Variable Definitions\"}\n        variable-definitions --&gt; build-variable-set[Build Variable Key-Value Pairs]\n        build-variable-set --&gt; local-variable-set@{shape: docs, label: \"Local Variable Set\"}\n    end\n\n    subgraph dataset [Metadata Dataset]\n        direction TB\n        selected --&gt; dataset-definitions@{shape: docs, label: \"Dataset Definitions\"}\n        local-variable-set --&gt; dto@{shape: docs, label: \"Dataset Transfer Object (DTO)\"}\n        dataset-definitions --&gt; dto\n    end\n\n</code></pre>"},{"location":"user-guide/metadata-schemas/#schema-definition-general-information","title":"Schema Definition (General Information)","text":"<p>The first several fields of <code>imsc</code> files, that are not <code>variables</code> or <code>schemas</code>, contain meta data of the schema file for admins and data-curators to identify the schema.  For ingestor, it contains the fields for ordering and matching schema files with the input data file to be ingested.</p> Field Name Field Type Description id string Unique id of the schema. Subsequent version of the same schema should have different ids. <code>uuid</code> is recommended, but there are not restrictions on the format. name string Human readable name. This field is only for admin to assign a meaningful name for the configuration. instrument string A list of relevant instrument names separated by comma(<code>,</code>). order integer The order of the schemas. Ingestor sorts the schemas using the value of the <code>order</code>. The lower the order is, the earlier the schema is evaluated for applicability. In case a file matches multiple schema selector, the schema with lower order number will be used. selector dict(object) | string A set of conditions for the schema to be matched with the input file."},{"location":"user-guide/metadata-schemas/#selector-example","title":"Selector Example","text":"<p>Example 1: The schema will be applied to files whose full path start with <code>/ess/data/coda</code> or <code>/ess/raw/coda</code> <pre><code>\"selector\" : {\n    \"or\" : [\n        \"filename:starts_with:/ess/data/coda\",\n        \"filename:starts_with:/ess/raw/coda\"\n    ]\n}\n</code></pre></p> <p>Example 2: The schema will be applied to files whose file name contains the word <code>coda</code>. <pre><code>\"selector\" : \"filename:contains:coda\"\n</code></pre></p> <p>Multiple selectors can be combined with <code>and</code> and <code>or</code> operators. Each selector can be a single string with three parts divided by colon(<code>:</code>), called <code>condensed selector</code>, or an object with the three equivalent fields, called <code>expanded selector</code>. </p> <p>The three components of each selector are: - source of the value to use of the first operand - operator - second operand.</p> <p>The syntax of the <code>condensed selector</code>: <pre><code>\"source_op_1:operator:op_2\"\n</code></pre></p> <p>The syntax of the <code>expanded selector</code>: <pre><code>{\n\"source\" : \"&lt;SOURCE_OF_THE_FIRST_OPERAND&gt;\",\n\"operator\" : \"&lt;OPERATOR&gt;\",\n\"operand_2\" : \"&lt;OPERAND_2&gt;\"\n}\n</code></pre></p> <p>Available Source Names:</p> Source Name Description filename Path to the input file (provided as input argument with option --nexus-file) datafile same as <code>filename</code> nexusfile same as <code>filename</code> <p>Available Operators:</p> Operator Name Description starts_with the value retrieved from the source should start with  operand_2 contains the value retrieved from the source should contain operand_2 <p>Operand two is always interpreted as string.</p>"},{"location":"user-guide/metadata-schemas/#variable-set-instruction","title":"Variable Set Instruction","text":"<p><code>variables</code> section of the <code>imsc</code> file is a set of variable definitions. It can contain as many definitions as the user needs in order to populate the metadata.</p> <p>Note</p> <p>The order of the definitions in the section is important, as the ingestor will follow it. Variables defined earlier can be used to define later ones.</p>"},{"location":"user-guide/metadata-schemas/#example_1","title":"Example","text":"<p><code>variables</code> section should look like this:</p> <pre><code>\"variables\" : {\n  \"&lt;VARIABLE_NAME_1&gt;\" : \"&lt;VARIABLE_DEFINITION_1&gt;\",\n  \"&lt;VARIABLE_NAME_2&gt;\" : \"&lt;VARIABLE_DEFINITION_2&gt;\",\n}\n</code></pre> <p><code>offline-ingestor</code> will store these key-value pairs in memory and will use them for evaluating the <code>schema</code> section to populate <code>SciCat</code> dataset.</p> <p>For example, given the following variables structure: <pre><code>\"variables\" : {\n  \"job_id\": {\n    \"source\": \"VALUE\",\n    \"value\": \"26e95e54\",\n    \"value_type\": \"string\"\n  },\n  \"owner_group\": {\n    \"source\": \"VALUE\",\n    \"value\": \"group_&lt;proposal_id&gt;\",\n    \"value_type\": \"string\"\n  },\n  \"access_groups\": {\n    \"source\": \"VALUE\",\n    \"value\": [\"group_1\", \"group_2\"],\n    \"value_type\": \"string[]\"\n  }\n}\n</code></pre></p> <p>The variables stored in memory will look like this: <pre><code>\"variables\" : {\n  \"job_id\": \"26e95e54\",\n  \"owner_group\": \"group_26e95e54\",\n  \"access_groups\": [\"group_1\", \"group_2\"],\n}\n</code></pre></p>"},{"location":"user-guide/metadata-schemas/#variable-definition","title":"Variable definition","text":"<p>Each variable definition has the following structure: <pre><code>\"&lt;variable_name&gt;\" : {\n    \"source\": \"&lt;source_of_value: NXS|SC|VALUE&gt;\",\n    \"value_type\": \"&lt;type_of_value: string|dict|list|date|ingeter|string[]&gt;\",\n    ...additional fields...\n}\n</code></pre></p> <p>This definition will define a variable of name  <code>variable_name</code> of type <code>type_of_value</code>  and the value of such variable will be retrieved from <code>source_of_value</code>.  Additional fields might be required depending on the <code>source</code>.</p>"},{"location":"user-guide/metadata-schemas/#value-types","title":"Value Types","text":"<p>The field value_type accept the following options:</p> Type Name Dtype in <code>python</code> Dtype in <code>typescript</code> Description string str string string[] list[str] array[string] list list array dict dict object integer int int float float float date str string ISO8601 format email str string link str string <p>Note</p> <p>value types are not necessarily <code>dtype</code>. For example, <code>date</code> specifies not only the dtype but also the regex that it should be a datetime in <code>ISO8601</code> format.</p> <p>Available <code>value-types</code> can be found in <code>_DtypeConvertingMap</code> in <code>scicat_dataset</code> module.</p>"},{"location":"user-guide/metadata-schemas/#sources","title":"Sources","text":"<p>Currently, the source field only accepts the following options:</p> Source Name Description NXS The nexus file specified as input argument with the option <code>--nexus-file</code>, i.e. the data file. SC The api of the SciCat instance of reference which the ingestor is configured to interact with the configuration option scicat.host. VALUE A static value specified directly in the definition <p>Available <code>sources</code> can be found in <code>scicat_metadata</code> module.  Here is more detailed description how to define variables from each <code>source</code>.</p>"},{"location":"user-guide/metadata-schemas/#nxs-nexus","title":"NXS (nexus)","text":"<p>A variable is retrieved from the nexus data file to be ingested. </p> <p>Syntax: <pre><code>\"&lt;variable_name&gt;\" : {\n    \"source\": \"NXS\",\n    \"path\": \"&lt;nexus_path&gt;\",\n    \"value_type\": \"&lt;type_of_value&gt;\",\n}\n</code></pre></p> <p>It means that the variable <code>variable_name</code> of type <code>type_of_value</code> will be retrieved from the <code>nexus_path</code> in the nexus file.</p> <p>Example: <pre><code>\"job_id\": {\n    \"source\": \"NXS\",\n    \"path\": \"/entry/entry_identifier_uuid\",\n    \"value_type\": \"string\"\n},\n</code></pre> The variable <code>job_id</code> is parsed as a <code>string</code> from the nexus file at <code>/entry/entry_identifier_uuid</code>.</p>"},{"location":"user-guide/metadata-schemas/#value","title":"VALUE","text":"<p>A variable value is directly defined in the definition itself. </p> <p>Syntax: <pre><code>\"&lt;variable_name&gt;\" : {\n    \"source\": \"VALUE\",\n    [\"operator\": \"&lt;OPERATOR_NAME&gt;\",]\n    \"value\": \"&lt;variable_value&gt;\",\n    \"value_type\": \"&lt;type_of_value&gt;\",\n}\n</code></pre> It means thatthe variable <code>variable_name</code> of type <code>type_of_value</code> has the value, <code>variable_value</code>. Additionally if the field <code>operator</code> is specified, the operator <code>operator_name</code> is applied to the value. The variable value can reference other variables that are previously defined i.e. <code>\"&lt;variable_name&gt;\"</code>.  Any matching variables between <code>&lt;&gt;</code> will be resolved by the ingestor.</p> <p>Example 1: <pre><code>\"pid\": {\n    \"source\": \"VALUE\",\n    \"value\": \"20.500.12269/23c40f04-de76-11ef-897f-bb426f5f764f\",\n    \"value_type\": \"string\"\n},\n</code></pre> A variable named <code>pid</code> has the value of <code>20.500.12269/23c40f04-de76-11ef-897f-bb426f5f764f</code> of dtype <code>string</code>.</p> <p>Example 2: <pre><code>\"pid\": {\n    \"source\": \"VALUE\",\n    \"value\": \"20.500.12269/&lt;job_id&gt;\",\n    \"value_type\": \"string\"\n},\n</code></pre> If there is variable <code>job_id</code> is with a <code>string</code> value, <code>cc59b538-de76-11ef-b192-d3d4305b1b90</code>,  the variable <code>pid</code> will have the value of <code>20.500.12269/cc59b538-de76-11ef-b192-d3d4305b1b90</code> of type <code>string</code>. </p>"},{"location":"user-guide/metadata-schemas/#value-operators","title":"Value Operators","text":"<p>A value definition can contain an additional field <code>operator</code> which specifies an operator to be applied to the value. Depending on the <code>operator</code>, <code>variable definition</code> may need extra fields. The current list of the operators available with their functionality is the following:</p> Operator Name Functionality Mandatory Extra Fields getitem Assuming the value is a <code>list</code> or a <code>dict</code>, select an element by the key, <code>field</code>. <code>field</code> str_replace Replace a pattern with a string within the value. <code>pattern</code>, <code>replacement</code> to_upper Convert the value to upper case to_lower Convert the value to lower case urlsafe Convert the value to a safe format to be used in a url join_with_space Assuming the value is an array, joins all the elements of the array with a space as separating character as a single string dirname Assuming the value is a file system path, returns the parent folder. It effectively eliminates the last element of the path. dirname-2 Assuming the value is a file system path, returns the grand-parent folder It effectively eliminates the last two elements of the path. filename Assuming the value is a file system path, returns the file name of the path. DO_NOTHING Returns the value itself.It is useful when you want to use the same value for multiple variables. <p>Here are examples of some complex operators that have extra fields:</p> <ul> <li>getitem</li> </ul> <p>If there is an existing variable <code>proposal_data</code> like this:   <pre><code>local_variables = {\n  \"proposal_data\": {\n      \"pi_firstname\" : \"John\",\n      \"pi_lastname\" : \"Doe\"\n  }\n}\n</code></pre></p> <p>The following variable definition will resolve to a variable <code>pi_firstname</code> with value <code>\"John\"</code>.</p> <pre><code>\"pi_firstname\": {\n  \"source\": \"VALUE\",\n  \"operator\": \"getitem\",\n  \"value\": \"&lt;proposal_data&gt;\",\n  \"field\" : \"pi_firstname\",\n  \"value_type\": \"string\"\n}\n</code></pre> <ul> <li>str_replace</li> </ul> <p>If there is an existing variable <code>raw_folder</code> like this:   <pre><code>local_variables = {\"raw_folder\": \"/ess/raw/instrument/year/proposal\"}\n</code></pre></p> <p>The following variable definition will resolve to a variable <code>source_folder</code> with value <code>\"/ess/data/instrument/year/proposal\"</code>.</p> <pre><code>\"source_folder\": {\n  \"source\": \"VALUE\",\n  \"operator\": \"str-replace\",\n  \"value\": \"&lt;raw_folder&gt;\",\n  \"pattern\": \"ess/raw\",\n  \"replacement\": \"ess/data\",\n  \"value_type\": \"string\"\n},\n</code></pre>"},{"location":"user-guide/metadata-schemas/#sc-scicat","title":"SC (SciCat)","text":"<p>A variable is retrieved from a SciCat instance. </p> <p>Syntax: <pre><code>\"&lt;VARIABLE_NAME&gt;\" : {\n    \"source\": \"SC\",\n    \"url\": \"&lt;ENDPOINT_URL&gt;\",\n    \"field\" : \"&lt;NAME_OF_THE_FIELD&gt;\n    \"value_type\": \"&lt;TYPE_OF_VALUE&gt;\",\n}\n</code></pre> It means a variable of  <code>variable_name</code> of type <code>type_of_value</code> will have the value retrieved from the <code>SciCat</code> instance, with the endpoint specified by the field <code>url</code>. If <code>field</code> is specified, it will extract the exact field from the results, otherwise it will assign the full object returned by scicat. The url is relative to the <code>SciCat</code> instance endpoint in the <code>configuration</code>.</p> <p>Example: <pre><code>\"proposal_data\": {\n  \"source\": \"SC\",\n  \"url\": \"proposals/&lt;proposal_id&gt;\",\n  \"field\" : \"\",\n  \"value_type\": \"dict\"\n},\n</code></pre></p> <p>The ingestor will put through a <code>GET</code> request to the endpoint <code>proposals</code> with <code>proposal_id</code>.  <code>proposal_id</code> should be defined already. <code>SciCat</code> should return a <code>dict</code> object containing all the information regarding the <code>proposal</code>. The variable proposal_data will have the returned object as its value.</p>"},{"location":"user-guide/metadata-schemas/#scicat-dataset-population-instruction","title":"Scicat Dataset Population Instruction","text":"<p>Under the <code>schema</code> section of the imsc file, <code>ingestor</code> finds how to populate the SciCat dataset fields using <code>variables</code>. <code>schema</code> section can contain arbitrary number of dataset definitions. The order of scicat dataset definitions is not important.</p> <p>Syntax: <pre><code>\"schema\" : {\n  \"&lt;ASSIGNMENT_NAME_1&gt;\" : \"&lt;FIELD_ASSIGNMENT_1&gt;\",\n  \"&lt;ASSIGNMENT_NAME_2&gt;\" : \"&lt;FIELD_ASSIGNMENT_2&gt;\",\n}\n</code></pre></p> <p>The names for the dataset definition, i.e. <code>\"&lt;ASSIGNMENT_NAME&gt;\"</code>, are not used by the ingestor.  They are meant to be human readable for the data curator.</p> <p>Example: <pre><code>\"schema\" : {\n  \"pid\": {\n    \"field_type\": \"high_level\",\n    \"machine_name\": \"pid\",\n    \"value\": \"&lt;pid&gt;\",\n    \"type\": \"string\"\n  },\n  \"type\" : {\n    \"field_type\": \"high_level\",\n    \"machine_name\": \"type\",\n    \"value\": \"raw\",\n    \"type\": \"string\"\n  },\n  \"proposal_id\": {\n    \"field_type\": \"high_level\",\n    \"machine_name\": \"proposalId\",\n    \"value\": \"&lt;proposal_id&gt;\",\n    \"type\": \"string\"\n  },\n  \"dataset_name\": {\n    \"field_type\": \"high_level\",\n    \"machine_name\": \"datasetName\",\n    \"value\": \"&lt;dataset_name&gt;\",\n    \"type\": \"string\"\n  }\n}\n</code></pre></p> <p>The dataset <code>pid</code> will use the variable <code>pid</code> as a value,  the dataset <code>type</code> will have a value of <code>\"raw\"</code>,  the dataset <code>proposalId</code> will use the variable <code>proposal_id</code> as a value,  and the dataset <code>datasetName</code> will use the variable <code>dataset_name</code> as a value.</p> <p>Note</p> <p>The <code>variable</code> name should be wrapped in <code>&lt;&gt;</code> so that <code>ingestor</code> recognizes it as a variable.</p>"},{"location":"user-guide/metadata-schemas/#field-assignment","title":"Field assignment","text":"<p>There are two types of scicat dataset definition:</p> <ul> <li>high level field</li> <li>scientific metadata field</li> </ul> <p>Each definition type is described in the following sub sections:</p>"},{"location":"user-guide/metadata-schemas/#high-level-field","title":"High Level Field","text":"<p>Syntax: <pre><code>\"&lt;ASSIGNMENT_NAME&gt;\": {\n  \"field_type\": \"high_level\",\n  \"machine_name\": \"&lt;SCICAT_DATASET_FIELD&gt;\",\n  \"value\": \"&lt;FIELD_VALUE&gt;\",\n  \"type\": \"&lt;FIELD_TYPE&gt;\"\n}\n</code></pre></p> <p>Dataset definition <code>&lt;ASSIGNMENT_NAME&gt;</code> will have value <code>&lt;FIELD_VALUE&gt;</code> of type <code>&lt;FIELD_TYPE&gt;</code>.  <code>&lt;SCICAT_DATASET_FIELD&gt;</code> is the field name in the SciCat dto(data transfer object).  Invalid field name will cause an error and the dataset entry will not be created by the <code>SciCat</code> backend.</p> <p>Example with static value: <pre><code>\"type\" : {\n  \"field_type\": \"high_level\",\n  \"machine_name\": \"type\",\n  \"value\": \"raw\",\n  \"type\": \"string\"\n},\n</code></pre> This assignment will assign the value <code>\"raw\"</code> to the dataset field <code>type</code>.</p> <p>Example using variable: <pre><code>\"creation_location\": {\n  \"field_type\": \"high_level\",\n  \"machine_name\": \"creationLocation\",\n  \"value\": \"ESS:CODA:&lt;instrument_name&gt;\",\n  \"type\": \"string\"\n},\n</code></pre></p> <p>Assuming that variable <code>instrument_name</code> has the value <code>dream</code>,  <code>createLocation</code> field in the dto will have the value, <code>\"ESS:CODE:dream\"</code>.</p>"},{"location":"user-guide/metadata-schemas/#scientific-metadata","title":"Scientific Metadata","text":"<p>Syntax: <pre><code>\"&lt;ASSIGNMENT_NAME&gt;\": {\n  \"field_type\": \"scientific_metadata\",\n  \"machine_name\": \"&lt;SCIENTIFIC_METADATA_MACHINE_FIELD_NAME&gt;\",\n  \"human_name\": \"&lt;SCIENTIFIC_METADATA_HUMAN_FIELD_NAME&gt;\",\n  \"value\": \"&lt;FIELD_VALUE&gt;\",\n  \"type\": \"&lt;VALUE_TYPE&gt;\"\n}\n</code></pre></p> <p>Ingestor will create a scientific metadata entry <code>assignment_name</code>,  named <code>&lt;SCIENTIFIC_METADATA_MACHINE_FIELD_NAME&gt;</code> (which is machine consumable)  with a value of <code>&lt;FIELD_VALUE&gt;</code> of type <code>&lt;VALUE_TYPE&gt;</code>.  It will also add the optional field <code>human_name</code> with the value <code>&lt;SCIENTIFIC_METADATA_HUMAN_FIELD_NAME&gt;</code>. which will appear in the <code>SciCat</code> frontend.</p> <p>Example 1: <pre><code>\"start_time\": {\n  \"field_type\": \"scientific_metadata\",\n  \"machine_name\": \"start_time\",\n  \"human_name\": \"Start Time\",\n  \"value\": \"&lt;start_time&gt;\",\n  \"type\": \"date\"\n},\n\"end_time\": {\n  \"field_type\": \"scientific_metadata\",\n  \"machine_name\": \"end_time\",\n  \"human_name\": \"End Time\",\n  \"value\": \"&lt;end_time&gt;\",\n  \"type\": \"date\"\n},\n</code></pre></p> <p>Assuming that variable <code>start_time</code> has value <code>\"2025-01-20 12:00:00\"</code>  and variable <code>end_time</code> has value <code>\"2025-01-20 12:30:00\"</code>,  the scientific metadata object will look like this:</p> <pre><code>\"scientific_metadata\" {\n  \"start_time\" : {\n    \"value\" : \"2025-01-20 12:00:00\"\n    \"unit\" : \"\"\n    \"human_name\" : \"Start time\"\n    \"type\" : \"date\"\n  },\n  \"end_time\" : {\n    \"value\" : \"2025-01-20 12:30:00\"\n    \"unit\" : \"\"\n    \"human_name\" : \"End time\"\n    \"type\" : \"date\"\n  }\n}\n</code></pre>"},{"location":"user-guide/offline-ingestor/","title":"Offline Ingestor","text":"<p><code>offline-ingestor</code> is a simple command line interface. When it is called, <code>offline-ingestor</code> ingests one data file according to a specific schema file (.imsc.yml).</p> <p>Tip</p> <p>If you are looking for a python interface that you can use to download/upload dataset occasionally, you should use <code>scitacean</code>.</p> <p> But... why? <code>offline-ingestor</code> is for <code>raw</code> datasets that are continuously produced by <code>daq</code> programs. It has very limited/low-level interface to communicate with scicat because it is expecting only certain types of files with known structure. (i.e. nexus file written by <code>ess-file-writer</code>.) Therefore it can't handle arbitrary type of files. <code>scitacean</code>, however, provides versatile high-level interfaces to process files and communicate with scicat with various authentication methods. <code>scitacean</code> can also validate a dataset before it is uploaded or after it is downloaded and <code>scitacean</code> will provide informative error messages. </p> <p>Note</p> <p>Currently the <code>offline ingestor</code> can only ingest <code>nexus(hdf5)</code> format. Other formats can be supported only if necessary. Please contact us on our github page or via your ESS, DMSC contact person.</p> <p>For more information about the schema files, the variables and schema sections of such files, please consult the schemas documentation</p>"},{"location":"user-guide/offline-ingestor/#how-to-run","title":"How to Run","text":"<p>In the production environment, the <code>offline ingestor</code> is executed by the <code>online ingestor</code> as a background process.  The <code>online ingestor</code> executes following command:  <pre><code>&lt;path to the scicat offline ingestor executable&gt; \\\n    -c &lt;full_path_to_the_configuration_file&gt; \\\n    --nexus-file &lt;full_path_to_the_nexus_data_file&gt;\n</code></pre></p> <p>Such command can also be run manually in a terminal in case of need (aka automatic ingestion failed) or troubleshooting.</p> <p>For example, if you want to ingest a file at <code>/ess/data/coda/2025/123321/raw/123321_000123456.hdf</code>, the command could be:</p> <pre><code>conda activate scicat-ingestor\nscicat_background_ingestor \\\n    -c /ess/services/scicat-ingestor/config/scicat_ingestor_config.yml \\\n    --nexus-file /ess/data/coda/2025/123321/raw/123321_000123456.hdf\n</code></pre> <p>or if you want to run the ingestor module directly,</p> <pre><code>/root/micromamba/envs/scicat-ingestor/bin/python -m /ess/services/scicat-ingestor/software/src/scicat_offline_ingestor.py \\\n    -c /ess/services/scicat-ingestor/config/scicat_ingestor_config.yml \\\n    --nexus-file /ess/data/coda/2025/123321/raw/123321_000123456.hdf\n</code></pre>"},{"location":"user-guide/offline-ingestor/#flow","title":"Flow","text":""},{"location":"user-guide/offline-ingestor/#ingestor-flow-diagram","title":"Ingestor Flow Diagram","text":"<pre><code>    flowchart TB\n\n    conf@{ shape: doc, label: \"Configuration File\" } --&gt; readconfig\n    nexus@{ shape: doc, label: \"Data File (Nexus)\" } --&gt; selectschema\n    schemas@{ shape: docs, \"Schema Definition Files\" } --&gt; loadschema\n\n    readconfig[Read the Configuration] --&gt; loadschema[Load Schema Files] --&gt; selectschema[Select Schema that matches the data file]\n    selectschema --&gt; schema@{ shape: doc, \"Selected Schema Definition\" }\n\n</code></pre>"},{"location":"user-guide/offline-ingestor/#ingestor-flow-description","title":"Ingestor Flow Description","text":"<ul> <li>read the configuration</li> <li>load the schema files</li> <li>select the schema file that matches the data file</li> <li>retrieve all the required values and assign them to internally defined <code>variables</code> according to the <code>_variables_</code> section of the schema file</li> <li>prepare <code>local representation of the dataset</code>, assigning field values according to the <code>_schema_</code> section of the schema file</li> <li>prepare <code>local representation of the file list</code> according to the provided configuration</li> <li>send a <code>POST</code> request to the SciCat instance of reference to create the <code>dataset</code></li> <li>send a <code>POST</code> request to the SciCat instance of reference to create the <code>origdatablock</code> containing the list of files</li> </ul>"},{"location":"user-guide/online-ingestor/","title":"Online Ingestor","text":"<p>The <code>online-ingestor</code> is an asynchronous active daemon program. It means <code>online-ingestor</code> pulls notification/information from a message broker and process them as it finds, instead of synchronously triggered by the file being written.</p> <p>Note</p> <p><code>scicat-ingestor</code> does not come with daemon installation helper. You will have to set it up by your self.</p> <p>Warning</p> <p>It is only tested on Ubuntu (&gt;=22.04) as we do not have any plans to support other types of OS.</p> Why is it not done by file-writer instead?  File-writer should be a very robust program as it is very critical for collecting data so it is better to have a single concern. As we keep file writing and file ingestion decoupled, even if either file-writer or scicat-ingestor fails, the other program can continue doing their job. Also they are maintained by different teams so for maintenance it is easier to keep the interface asynchronous rather than building a monolithic program.   <p>Whenever it can pull a notification about a new dataset, it spawns a background process where the <code>offline-ingestor</code> ingests the file.</p> <p><code>online-ingestor</code> spawns only a limited number of <code>offline-ingestor</code> processes. Whenever the number of <code>offline-ingestor</code> processes reaches the limitation, it stops and wait until certain background processes are done. The number of processes limitation is configurable by <code>max_offline_ingestors</code> and <code>offline_ingestor_wait_time</code>.</p> <p>Note</p> <p>The <code>scicat-ingestor</code> is developed for ESS specifically therefore it only has support to <code>kafka</code> broker and expects specific flatbuffer schema type(wfdn) used by our filewriter. Generalization and adoption of different delivery and messaging system will be considered on a per-request base. If you are interested in using ingestor with other frameworks, please contact us on our issue board or directly to the maintainers.</p>"},{"location":"user-guide/online-ingestor/#how-to-run","title":"How to Run","text":"<p>As <code>online-ingestor</code> is the main purpose of this project, it has an entry-point of script as <code>scicat_ingestor</code>.</p> <p>See getting started page</p> <p>Or you can also run it as a module or as a script itself.</p> <p>For example: <pre><code>&lt;path_to_the_selected_python_executable&gt; \\\n&lt;full_path_to_the_ingestor_executable_folder&gt;/scicat_online_ingestor.py \\\n-c &lt;full_path_to_the_configuration_file&gt;\n</code></pre></p> <p>In the case of the ESS test environment, the command looks like this: <pre><code>/root/micromamba/envs/scicat-ingestor/bin/python \\\n/ess/services/scicat-ingestor/software/src/scicat_online_ingestor.py \\\n-c /ess/services/scicat-ingestor/config/scicat_ingestor_config.yml\n</code></pre></p>"},{"location":"user-guide/online-ingestor/#configuration","title":"Configuration","text":"<p>See configuration page for more details.</p> <p>Online ingestor uses only the following sections of the configurations: - ingestion - kafka - logging</p> <p>The rest is simply passed to the offline ingestor from the file.</p> <p>See ADR-000#configuration for why <code>online-ingesetor</code> and <code>offline-ingestor</code> share the same configuration file.</p>"},{"location":"user-guide/overview/","title":"Overview","text":"<p>As mentioned in the ADR-001, <code>scicat-ingestor</code> has two main components, <code>online-ingestor</code> and <code>offline-ingestor</code>.</p> <p><code>online-ingestor</code> is a daemonic program that runs <code>offline-ingestor</code> in real-time. In other words, <code>online-ingestor</code> is higher level program than <code>offline-ingestor</code>.</p> <p>This page has various diagrams that shows how the ingestion flow or sequence is done. You can find the relationship between <code>online</code> and <code>offline</code> ingestor from the diagram.</p> <p>See online-ingestor page and offline-ingestor page for more details.</p>"},{"location":"user-guide/overview/#infrastructure-around-scicat-ingestor","title":"Infrastructure around Scicat Ingestor","text":"<p><code>scicat-ingestor</code> is written for specific infrastructure setup like below:</p> <pre><code>---\ntitle: Infrastructure around Scicat Ingestor\n---\ngraph LR\n    filewriter@{ shape: processes, label: \"File Writers\" } -.write file.-&gt; storage[(Storage)]\n    filewriter --report (wrdn)--&gt; kafkabroker[Kafka Broker]\n    ingestor[Scicat Ingestor] -.subscribe (wrdn).-&gt; kafkabroker\n    storage -.read file.-&gt; ingestor\n    ingestor --report--&gt; log[Gray Log]\n\n</code></pre>"},{"location":"user-guide/overview/#ingestor-flow-chart-bird-eye-view","title":"Ingestor Flow Chart - Bird Eye View","text":""},{"location":"user-guide/overview/#ingestor-flow-chart-detail","title":"Ingestor Flow Chart - Detail","text":"<pre><code>---\ntitle: Ingestor Flow Chart - Detail\n---\nflowchart LR\n\n    subgraph online [Online Ingestor]\n        direction TB\n        connect-to-kafka[Connect to Kafka Cluster] --&gt; subscription[Subscribe to Instrument Topics]\n        subscription --&gt; wait[Wait for Next Messages]\n        wait --&gt; done{{Done Writing Message?}}\n        done --&gt; |No| wait\n        done --&gt; |Yes| max-process{{Maximum Offline Ingestor Running?}}\n        max-process --&gt; |Yes| wait-running@{ shape: delay , label: \"Wait for previous ingestors\"}\n        wait-running --&gt; max-process\n        max-process --&gt; |No| start@{shape: circle, label: \"Start Offline Ingestor\"}\n        start --&gt; wait\n    end\n\n    subgraph offline [Offline Ingestor]\n        direction TB\n        start-offline@{shape: circle, label: \"Start Offline Ingestor\"}\n        start-offline --&gt; load-schema[Load Schema]\n        load-schema --&gt; select[Select Schema]\n        select --&gt; open[Open Nexus File, Event Data]\n        open --&gt; variable[Define Variables]\n        variable --&gt; populate[Populate Local Dataset]\n        populate --&gt; create[Create Dataset on Scicat]\n        create --&gt; create-origdataset[Create OrigDataset on Scicat]\n        create-origdataset --&gt; stop@{shape: dbl-circ, label: \"Finish Offline Ingestor\"}\n\n    end\n\n    online --&gt; offline\n\n    style start fill:green,stroke-width:4px,opacity:0.5;\n    style start-offline fill:green,stroke-width:4px,opacity:0.5;\n\n</code></pre>"},{"location":"user-guide/overview/#ingestor-sequence-chart","title":"Ingestor Sequence Chart","text":"<pre><code>---\ntitle: File Ingesting Sequence\n---\n\nsequenceDiagram\n  create participant File Writer\n  create actor File\n  File Writer --&gt; File: File Written\n  loop Ingest Files\n    Ingestor --&gt;&gt; Kafka Broker: Subscribe(listening to writing done - wrdn)\n    Kafka Broker -&gt;&gt; Ingestor: Writing Done Message (wrdn)\n    Note over Ingestor: Parse writing done message\n    Ingestor -&gt;&gt; File: Check file\n    opt\n        Ingestor -&gt;&gt; File: Parse Metadata\n    end\n    Note over Ingestor: Wrap files and metadata as Scicat Dataset\n    critical\n        Ingestor -&gt;&gt; Scicat: Ingest File\n    end\n  end\n\n</code></pre>"}]}